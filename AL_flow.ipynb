{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedModel:\n",
    "    model = None\n",
    "    iteration = 0\n",
    "    evaluation_results = None\n",
    "    training_results = None\n",
    "    # train_data_ptr, val_data_ptr\n",
    "    \n",
    "class AnnotatedProject:\n",
    "    train_dataset = None\n",
    "    val_dataset = None\n",
    "    quality_score = None\n",
    "\n",
    "class TrainDataset:\n",
    "    pass\n",
    "\n",
    "class ValDataset:\n",
    "    pass\n",
    "\n",
    "class NewSamplingAlgorithm:\n",
    "    def sample_data(self, unlabeled_data):\n",
    "        dataset = self._sample_data(unlabeled_data)\n",
    "        self.dataset_id = dataset.id\n",
    "    \n",
    "    def _sample_data(self, unlabeled_data):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, model=None):\n",
    "        self.metrics = self._evaluate(model)\n",
    "\n",
    "    def _evaluate(self, model):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def annotate_data(unlabeled_data, model=None, **kwargs):\n",
    "    pass\n",
    "\n",
    "def train_model(train_data, val_data, retrain, **kwargs):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UX\n",
    "\n",
    "def view_model_benchmark_dashboard(TRAINED_MODELS):\n",
    "    # generate on the fly if possible\n",
    "    pass\n",
    "\n",
    "def view_sampling_strategy_metrics():\n",
    "    pass\n",
    "\n",
    "def view_full_history_dashboard():\n",
    "    pass\n",
    "\n",
    "def import_new_data():\n",
    "    pass\n",
    "\n",
    "MANUAL_REVIEW = lambda: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODELS = []\n",
    "ANNOTATED_PROJECTS = []\n",
    "\n",
    "unlabeled_data = import_new_data()\n",
    "\n",
    "prev_sampling_algorithm = None\n",
    "prev_val_dataset = None\n",
    "\n",
    "\n",
    "while True:\n",
    "    train_dataset = TrainDataset()\n",
    "    val_dataset = ValDataset()\n",
    "\n",
    "    # Step 1. Tune sampling strategy\n",
    "    model_i = TRAINED_MODELS.pick_best_model() or None\n",
    "    satisfied = False\n",
    "    while not satisfied:\n",
    "        sampling_strategy = optimize_sampling_strategy(\n",
    "            unlabeled_data,\n",
    "            TRAINED_MODELS,\n",
    "            ANNOTATED_PROJECTS,\n",
    "            prev_sampling_algorithm,  # or SAMPLING_ALGORITHMS\n",
    "            prev_val_dataset,\n",
    "            ...,\n",
    "        )\n",
    "        sampling_strategy = MANUAL_REVIEW(sampling_strategy)\n",
    "        sampling_algorithm = NewSamplingAlgorithm(sampling_strategy, unlabeled_data, TRAINED_MODELS, ANNOTATED_PROJECTS, ...)\n",
    "        unlabeled_val = sampling_algorithm.sample_data(unlabeled_data)\n",
    "        val_dataset += annotate_data(unlabeled_val, model_i, ...)\n",
    "        unlabeled_data -= val_dataset\n",
    "        sampling_algorithm.evaluate(sampling_algorithm, model_i)\n",
    "        satisfied = MANUAL_REVIEW()\n",
    "        prev_sampling_algorithm = sampling_algorithm\n",
    "        prev_val_dataset = val_dataset\n",
    "        if not satisfied:\n",
    "            train_dataset += val_dataset\n",
    "\n",
    "    # Step 2. Annotate train data\n",
    "    unlabeled_train = sampling_algorithm.sample_data(unlabeled_data)\n",
    "    train_dataset += annotate_data(unlabeled_train, model_i, ...)\n",
    "    unlabeled_data -= train_dataset\n",
    "    project_i = AnnotatedProject(train_dataset, val_dataset)\n",
    "    ANNOTATED_PROJECTS.append(project_i)\n",
    "\n",
    "    # Step 3. Retrain models\n",
    "    satisfied = False\n",
    "    while not satisfied:\n",
    "        model_i : TrainedModel = train_model(train_dataset, val_dataset, retrain=True, ...)\n",
    "        TRAINED_MODELS.append(model_i)\n",
    "        satisfied = MANUAL_REVIEW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualSamplingWithAI:\n",
    "    pass\n",
    "\n",
    "\n",
    "def optimize_sampling_strategy(model, project, unlabeled_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
