{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval, Params\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "from model_benchmark import metrics, utils\n",
    "from model_benchmark.metric_provider import MetricProvider, METRIC_NAMES\n",
    "from model_benchmark import metric_provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt_path = \"cocoGt_remap.json\"\n",
    "cocoDt_path = \"data/model-benchmark/COCO 2017 val (YOLOv8-L, conf-0.01)/cocoDt.json\"\n",
    "eval_data_path = \"eval_data_conf-0.01.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt = COCO(cocoGt_path)\n",
    "cocoDt = cocoGt.loadRes(cocoDt_path)\n",
    "# cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "\n",
    "import pickle\n",
    "with open(eval_data_path, 'rb') as f:\n",
    "    eval_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(metric_provider)\n",
    "m_full = metric_provider.MetricProvider(eval_data['matches'], eval_data['coco_metrics'], eval_data['params'], cocoGt, cocoDt)\n",
    "m_full.base_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-optimal conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_profile, f1s = m_full.confidence_score_profile()\n",
    "# score_profile = m_full.confidence_score_profile_v0()\n",
    "f1_optimal_conf, best_f1 = m_full.get_f1_optimal_conf(score_profile)\n",
    "print(f\"F1-Optimal confidence: {f1_optimal_conf:.4f} with f1: {best_f1:.4f}\")\n",
    "\n",
    "matches_thresholded = metric_provider.filter_by_conf(eval_data['matches'], f1_optimal_conf)\n",
    "m = metric_provider.MetricProvider(matches_thresholded, eval_data['coco_metrics'], eval_data['params'], cocoGt, cocoDt)\n",
    "m.base_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Metrics\n",
    "base_metrics = m.base_metrics()\n",
    "r = list(base_metrics.values())\n",
    "theta = [metric_provider.METRIC_NAMES[k] for k in base_metrics.keys()]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=r+[r[0]], theta=theta+[theta[0]], fill='toself', name='Overall Metrics',\n",
    "    hovertemplate='%{theta}: %{r:.2f}<extra></extra>',\n",
    "))\n",
    "fig.update_layout(polar=dict(radialaxis=dict(range=[0., 1.]),\n",
    "                             angularaxis=dict(rotation=90, direction='clockwise')),\n",
    "                  title=\"Overall Metrics\", width=600, height=500,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = m.prediction_table()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the optimal Confidence Threshold is calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(score_profile)\n",
    "df.columns = ['scores', 'Precision', 'Recall', 'F1']\n",
    "\n",
    "# downsample\n",
    "if len(df) > 5000:\n",
    "    df_down = df.iloc[::len(df)//1000]\n",
    "else:\n",
    "    df_down = df\n",
    "\n",
    "color_map = {\n",
    "    'Precision': '#1f77b4',\n",
    "    'Recall': 'orange',\n",
    "}\n",
    "fig = px.line(df_down, x='scores', y=['Precision', 'Recall', 'F1'], title='Confidence Score Profile',\n",
    "              labels={'value': 'Value', 'variable': 'Metric', 'scores': 'Confidence Score'},\n",
    "              width=None, height=500,\n",
    "              color_discrete_map=color_map)\n",
    "fig.update_layout(yaxis=dict(range=[0, 1]),\n",
    "                  xaxis=dict(range=[0, 1], tick0=0, dtick=0.1))\n",
    "\n",
    "# Add vertical line for the best threshold\n",
    "fig.add_shape(type=\"line\", x0=f1_optimal_conf, x1=f1_optimal_conf, y0=0, y1=best_f1, line=dict(color=\"gray\", width=2, dash=\"dash\"))\n",
    "fig.add_annotation(x=f1_optimal_conf, y=best_f1+0.04, text=f\"F1-optimal threshold: {f1_optimal_conf:.2f}\", showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "f1s_down = f1s[:,::f1s.shape[1]//1000]\n",
    "iou_names = list(map(lambda x: str(round(x,2)), m.iouThrs.tolist()))\n",
    "df = pd.DataFrame(np.concatenate([df_down['scores'].values[:,None], f1s_down.T], 1), columns=['scores'] + iou_names)\n",
    "fig = px.line(df, x='scores', y=iou_names, title='F1-Score at different IoU Thresholds',\n",
    "              labels={'value': 'Value', 'variable': 'IoU threshold', 'scores': 'Confidence Score'},\n",
    "              color_discrete_sequence=px.colors.sequential.Viridis,\n",
    "              width=None, height=500)\n",
    "fig.update_layout(yaxis=dict(range=[0, 1]),\n",
    "                  xaxis=dict(range=[0, 1], tick0=0, dtick=0.1))\n",
    "\n",
    "# add annotations for maximum F1-Score for each IoU threshold\n",
    "for i, iou in enumerate(iou_names):\n",
    "    argmax_f1 = f1s[i].argmax()\n",
    "    max_f1 = f1s[i][argmax_f1]\n",
    "    score = score_profile['scores'][argmax_f1]\n",
    "    fig.add_annotation(x=score, y=max_f1, text=f'Best score: {score:.2f}', showarrow=True, arrowhead=1, arrowcolor='black', ax=0, ay=-30)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome counts\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=[m.TP_count], y=[\"Outcome\"], name='TP', orientation='h', marker=dict(color='#1fb466')))\n",
    "fig.add_trace(go.Bar(x=[m.FN_count], y=[\"Outcome\"], name='FN', orientation='h', marker=dict(color='#dd3f3f')))\n",
    "fig.add_trace(go.Bar(x=[m.FP_count], y=[\"Outcome\"], name='FP', orientation='h', marker=dict(color='#d5a5a5')))\n",
    "fig.update_layout(barmode='stack', title=\"Outcome Counts\",\n",
    "                  width=600, height=300)\n",
    "fig.update_xaxes(title_text=\"Count\")\n",
    "fig.update_yaxes(tickangle=-90)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_metrics_df = m.per_class_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision and Recall bar chart\n",
    "per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"f1\")\n",
    "\n",
    "blue_color = '#1f77b4'\n",
    "orange_color = '#ff7f0e'\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(y=per_class_metrics_df_sorted[\"precision\"], x=per_class_metrics_df_sorted[\"category\"], name='Precision', marker=dict(color=blue_color)))\n",
    "fig.add_trace(go.Bar(y=per_class_metrics_df_sorted[\"recall\"], x=per_class_metrics_df_sorted[\"category\"], name='Recall', marker=dict(color=orange_color)))\n",
    "fig.update_layout(barmode='group', title=\"Per-class Precision and Recall (Sorted by F1)\")\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Value\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision bar chart\n",
    "# per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"precision\")\n",
    "fig = px.bar(per_class_metrics_df_sorted, x='category', y='precision', title=\"Per-class Precision (Sorted by F1)\",\n",
    "             color='precision', color_continuous_scale='Plasma')\n",
    "if len(per_class_metrics_df_sorted) <= 20:\n",
    "    fig.update_traces(text=per_class_metrics_df_sorted[\"precision\"].round(2), textposition='outside')\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Precision\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision bar chart\n",
    "# per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"recall\")\n",
    "fig = px.bar(per_class_metrics_df_sorted, x='category', y='recall', title=\"Per-class Recall (Sorted by F1)\",\n",
    "             color='recall', color_continuous_scale='Plasma')\n",
    "if len(per_class_metrics_df_sorted) <= 20:\n",
    "    fig.update_traces(text=per_class_metrics_df_sorted[\"recall\"].round(2), textposition='outside')\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Recall\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve = m.pr_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve\n",
    "fig = px.line(x=m.recThrs, y=pr_curve.mean(-1), title=\"Precision-Recall Curve\",\n",
    "                labels={'x': 'Recall', 'y': 'Precision'},\n",
    "                width=600, height=500)\n",
    "fig.data[0].name = \"Model\"\n",
    "fig.data[0].showlegend = True\n",
    "fig.update_traces(fill='tozeroy', line=dict(color='#1f77b4'))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=m.recThrs,\n",
    "        y=[1]*len(m.recThrs),\n",
    "        name=\"Perfect\",\n",
    "        line=dict(color='orange', dash='dash'),\n",
    "        showlegend=True\n",
    "    )\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=f\"mAP = {base_metrics['mAP']:.2f}\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.98, y=0.92,\n",
    "    showarrow=False,\n",
    "    bgcolor=\"white\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve per-class\n",
    "df = pd.DataFrame(pr_curve, columns=m.cat_names)\n",
    "\n",
    "fig = px.line(df, x=m.recThrs, y=df.columns, title=\"Precision-Recall Curve per Class\",\n",
    "              labels={\"x\": \"Recall\", \"value\": \"Precision\", \"variable\": \"Category\"},\n",
    "              color_discrete_sequence=px.colors.qualitative.Prism, width=800, height=600)\n",
    "\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.update_xaxes(range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = m.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cat_names = m.cat_names\n",
    "none_name = \"(None)\"\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(np.log(confusion_matrix), index=cat_names + [none_name], columns=cat_names + [none_name])\n",
    "fig = px.imshow(confusion_matrix_df, labels=dict(x=\"Ground Truth\", y=\"Predicted\", color=\"Count\"), title=\"Confusion Matrix (log-scale)\",\n",
    "                width=1000, height=1000)\n",
    "\n",
    "# Hover text\n",
    "fig.update_traces(customdata=confusion_matrix,\n",
    "                  hovertemplate='Count: %{customdata}<br>Predicted: %{y}<br>Ground Truth: %{x}')\n",
    "\n",
    "# Text on cells\n",
    "if len(cat_names) <= 20:\n",
    "    fig.update_traces(text=confusion_matrix,\n",
    "                      texttemplate=\"%{text}\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently Confused Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of confusion as bar chart\n",
    "confused_df = m.frequently_confused(confusion_matrix, topk_pairs=20)\n",
    "confused_name_pairs = confused_df[\"category_pair\"]\n",
    "confused_prob = confused_df[\"probability\"]\n",
    "x_labels = [f\"{pair[0]} - {pair[1]}\" for pair in confused_name_pairs]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=x_labels, y=confused_prob, marker=dict(color=confused_prob, colorscale=\"Reds\")))\n",
    "fig.update_layout(title=\"Frequently confused class pairs\", xaxis_title=\"Class pair\", yaxis_title=\"Probability\")\n",
    "fig.update_traces(text=confused_prob.round(2))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IoU Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "nbins = 40\n",
    "fig.add_trace(go.Histogram(x=m.ious, nbinsx=nbins))\n",
    "fig.update_layout(title=\"IoU Distribution\", xaxis_title=\"IoU\", yaxis_title=\"Count\",\n",
    "                  width=600, height=500)\n",
    "\n",
    "# Add annotation for mean IoU as vertical line\n",
    "mean_iou = m.ious.mean()\n",
    "y1 = len(m.ious) // nbins\n",
    "fig.add_shape(type=\"line\", x0=mean_iou, x1=mean_iou, y0=0, y1=y1, line=dict(color=\"orange\", width=2, dash=\"dash\"))\n",
    "fig.add_annotation(x=mean_iou, y=y1, text=f\"Mean IoU: {mean_iou:.2f}\", showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curve (only positive predictions)\n",
    "true_probs, pred_probs = m_full.calibration_metrics.calibration_curve()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pred_probs, y=true_probs, mode='lines+markers', name='Calibration plot (Model)', \n",
    "                         line=dict(color='blue'), marker=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Perfectly calibrated', \n",
    "                         line=dict(color='orange', dash='dash')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Calibration Curve (only positive predictions)',\n",
    "    xaxis_title='Confidence Score',\n",
    "    yaxis_title='Fraction of True Positives',\n",
    "    legend=dict(x=0.6, y=0.1),\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(score_profile)\n",
    "df.columns = ['scores', 'Precision', 'Recall', 'F1']\n",
    "\n",
    "# downsample\n",
    "if len(df) > 5000:\n",
    "    df_down = df.iloc[::len(df)//1000]\n",
    "else:\n",
    "    df_down = df\n",
    "\n",
    "color_map = {\n",
    "    'Precision': '#1f77b4',\n",
    "    'Recall': 'orange',\n",
    "}\n",
    "fig = px.line(df_down, x='scores', y=['Precision', 'Recall', 'F1'], title='Confidence Score Profile',\n",
    "              labels={'value': 'Value', 'variable': 'Metric', 'scores': 'Confidence Score'},\n",
    "              width=None, height=500,\n",
    "              color_discrete_map=color_map)\n",
    "fig.update_layout(yaxis=dict(range=[0, 1]),\n",
    "                  xaxis=dict(range=[0, 1], tick0=0, dtick=0.1))\n",
    "\n",
    "# Add vertical line for the best threshold\n",
    "fig.add_shape(type=\"line\", x0=f1_optimal_conf, x1=f1_optimal_conf, y0=0, y1=best_f1, line=dict(color=\"gray\", width=2, dash=\"dash\"))\n",
    "fig.add_annotation(x=f1_optimal_conf, y=best_f1+0.04, text=f\"F1-optimal threshold: {f1_optimal_conf:.2f}\", showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "f1s_down = f1s[:,::f1s.shape[1]//1000]\n",
    "iou_names = list(map(lambda x: str(round(x,2)), m.iouThrs.tolist()))\n",
    "df = pd.DataFrame(np.concatenate([df_down['scores'].values[:,None], f1s_down.T], 1), columns=['scores'] + iou_names)\n",
    "fig = px.line(df, x='scores', y=iou_names, title='F1-Score at different IoU Thresholds',\n",
    "              labels={'value': 'Value', 'variable': 'IoU threshold', 'scores': 'Confidence Score'},\n",
    "              color_discrete_sequence=px.colors.sequential.Viridis,\n",
    "              width=None, height=500)\n",
    "fig.update_layout(yaxis=dict(range=[0, 1]),\n",
    "                  xaxis=dict(range=[0, 1], tick0=0, dtick=0.1))\n",
    "\n",
    "# add annotations for maximum F1-Score for each IoU threshold\n",
    "for i, iou in enumerate(iou_names):\n",
    "    argmax_f1 = f1s[i].argmax()\n",
    "    max_f1 = f1s[i][argmax_f1]\n",
    "    score = score_profile['scores'][argmax_f1]\n",
    "    fig.add_annotation(x=score, y=max_f1, text=f'Best score: {score:.2f}', showarrow=True, arrowhead=1, arrowcolor='black', ax=0, ay=-30)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of confidence scores (TP vs FP)\n",
    "scores_tp, scores_fp = m_full.calibration_metrics.scores_tp_and_fp(iou_idx=0)\n",
    "\n",
    "tp_y, tp_x = np.histogram(scores_tp, bins=40, range=[0,1])\n",
    "fp_y, fp_x = np.histogram(scores_fp, bins=40, range=[0,1])\n",
    "dx = (tp_x[1] - tp_x[0])/2\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=scores_fp, name='FP', marker=dict(color='#dd3f3f'), opacity=0.5, xbins=dict(size=0.025)))\n",
    "fig.add_trace(go.Histogram(x=scores_tp, name='TP', marker=dict(color='#1fb466'), opacity=0.5, xbins=dict(size=0.025)))\n",
    "fig.add_trace(go.Scatter(x=tp_x+dx, y=tp_y, mode='lines', name='TP', line=dict(color='#1fb466', width=3)))\n",
    "fig.add_trace(go.Scatter(x=fp_x+dx, y=fp_y, mode='lines', name='FP', line=dict(color='#dd3f3f', width=3)))\n",
    "\n",
    "# Best threshold\n",
    "fig.add_shape(type=\"line\", x0=f1_optimal_conf, x1=f1_optimal_conf, y0=0, y1=tp_y.max()*1.3, line=dict(color=\"orange\", width=1, dash=\"dash\"))\n",
    "fig.add_annotation(x=f1_optimal_conf, y=tp_y.max()*1.3, text=f\"F1-optimal threshold: {f1_optimal_conf:.2f}\", showarrow=False)\n",
    "\n",
    "fig.update_layout(barmode='overlay', title=\"Histogram of Confidence Scores (TP vs FP)\",\n",
    "                  width=800, height=500)\n",
    "fig.update_xaxes(title_text=\"Confidence Score\", range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"Count\", range=[0, tp_y.max()*1.3])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP per-class\n",
    "ap_per_class = m.coco_precision[:, :, :, 0, 2].mean(axis=(0, 1))\n",
    "# Per-class Average Precision (AP)\n",
    "fig = px.scatter_polar(r=ap_per_class, theta=m.cat_names, title=\"Per-class Average Precision (AP)\",\n",
    "                       labels=dict(r=\"Average Precision\", theta=\"Category\"),\n",
    "                       width=800, height=800,\n",
    "                       range_r=[0, 1])\n",
    "# fill points\n",
    "fig.update_traces(fill='toself')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Counts\n",
    "iou_thres = 0\n",
    "\n",
    "tp = m.true_positives[:,iou_thres]\n",
    "fp = m.false_positives[:,iou_thres]\n",
    "fn = m.false_negatives[:,iou_thres]\n",
    "\n",
    "# normalize\n",
    "support = tp + fn\n",
    "tp_rel = tp / support\n",
    "fp_rel = fp / support\n",
    "fn_rel = fn / support\n",
    "\n",
    "# sort by f1\n",
    "sort_scores = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "K = len(m.cat_names)\n",
    "sort_indices = np.argsort(sort_scores)\n",
    "cat_names_sorted = [m.cat_names[i] for i in sort_indices]\n",
    "tp_rel, fn_rel, fp_rel = tp_rel[sort_indices], fn_rel[sort_indices], fp_rel[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked per-class counts\n",
    "data = {\n",
    "    \"count\": np.concatenate([tp_rel, fn_rel, fp_rel]),\n",
    "    \"type\": [\"TP\"]*K + [\"FN\"]*K + [\"FP\"]*K,\n",
    "    \"category\": cat_names_sorted*3\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "color_map = {\n",
    "    'TP': '#1fb466',\n",
    "    'FN': '#dd3f3f',\n",
    "    'FP': '#d5a5a5'\n",
    "}\n",
    "fig = px.bar(df, x=\"category\", y=\"count\", color=\"type\", title=\"Per-class Outcome Counts\",\n",
    "             labels={'count': 'Total Count', \"category\": \"Category\"},\n",
    "             color_discrete_map=color_map)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked per-class counts\n",
    "data = {\n",
    "    \"count\": np.concatenate([tp[sort_indices], fn[sort_indices], fp[sort_indices]]),\n",
    "    \"type\": [\"TP\"]*K + [\"FN\"]*K + [\"FP\"]*K,\n",
    "    \"category\": cat_names_sorted*3\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "color_map = {\n",
    "    'TP': '#1fb466',\n",
    "    'FN': '#dd3f3f',\n",
    "    'FP': '#d5a5a5'\n",
    "}\n",
    "fig = px.bar(df, x=\"category\", y=\"count\", color=\"type\", title=\"Per-class Outcome Counts\",\n",
    "             labels={'count': 'Total Count', \"category\": \"Category\"},\n",
    "             color_discrete_map=color_map)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corr-plot (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-recall for area bin\n",
    "n_bins = 40\n",
    "max_area = 0.4\n",
    "\n",
    "tps = np.zeros(n_bins)\n",
    "fps = np.zeros(n_bins)\n",
    "fns = np.zeros(n_bins)\n",
    "\n",
    "def get_area(match, cocoGt: COCO):\n",
    "    img_info = cocoGt.imgs[match['image_id']]\n",
    "    img_w, img_h = img_info['width'], img_info['height']\n",
    "    img_area = img_w * img_h\n",
    "\n",
    "    gt_bbox = cocoGt.anns[match['gt_id']]\n",
    "    gt_area_abs = gt_bbox['bbox'][2] * gt_bbox['bbox'][3]\n",
    "    gt_area = gt_area_abs / img_area\n",
    "    # dt_bbox = cocoDt.anns[match['dt_id']]\n",
    "    # dt_area_abs = dt_bbox['area']\n",
    "    # dt_area = dt_area_abs / img_area\n",
    "    return gt_area\n",
    "\n",
    "tp_areas = [get_area(m, cocoGt) for m in m.tp_matches]\n",
    "fn_areas = [get_area(m, cocoGt) for m in m.fn_matches]\n",
    "tp_areas = np.sqrt([a for a in tp_areas if a < max_area])\n",
    "fn_areas = np.sqrt([a for a in fn_areas if a < max_area])\n",
    "\n",
    "area_bins = np.linspace(0, np.sqrt(max_area), n_bins+1)\n",
    "tp_hist = np.histogram(tp_areas, bins=area_bins)[0]\n",
    "fn_hist = np.histogram(fn_areas, bins=area_bins)[0]\n",
    "recall = tp_hist / (tp_hist + fn_hist)\n",
    "bin_dt = (area_bins[1] - area_bins[0]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=area_bins[:-1]+bin_dt, y=recall, title=\"Recall vs Area (sqrt)\", labels={'x': 'Area (sqrt)', 'y': 'Recall'},\n",
    "             color=recall, color_continuous_scale='Viridis')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of areas TP vs FN\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=tp_areas, name='TP', marker=dict(color='#1fb466'), opacity=0.5, xbins=dict(size=0.01)))\n",
    "fig.add_trace(go.Histogram(x=fn_areas, name='FN', marker=dict(color='#dd3f3f'), opacity=0.5, xbins=dict(size=0.01)))\n",
    "fig.update_layout(barmode='overlay', title=\"Histogram of Areas (TP vs FN)\",\n",
    "                  width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = 'book'\n",
    "class_idx = m.cat_names.index(class_name)\n",
    "\n",
    "ap_score = m.coco_precision[:, :, class_idx, 0, 2].mean(axis=(0, 1))\n",
    "print(f\"AP for {class_name}: {ap_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_thres = 0\n",
    "tp_count = m.true_positives[class_idx, iou_thres]\n",
    "fp_count = m.false_positives[class_idx, iou_thres]\n",
    "fn_count = m.false_negatives[class_idx, iou_thres]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=[tp_count], y=[\"Outcome\"], name='TP', orientation='h', marker=dict(color='#1fb466')))\n",
    "fig.add_trace(go.Bar(x=[fn_count], y=[\"Outcome\"], name='FN', orientation='h', marker=dict(color='#dd3f3f')))\n",
    "fig.add_trace(go.Bar(x=[fp_count], y=[\"Outcome\"], name='FP', orientation='h', marker=dict(color='#d5a5a5')))\n",
    "fig.update_layout(barmode='stack', title=\"Outcome Counts: \" + class_name,\n",
    "                  width=600, height=300)\n",
    "fig.update_xaxes(title_text=\"Count\")\n",
    "fig.update_yaxes(tickangle=-90)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_metrics = per_class_metrics_df[per_class_metrics_df[\"category\"] == class_name].to_dict(orient='records')[0]\n",
    "print(f\"F1-Score for {class_name}: {cls_metrics['f1']:.4f}\")\n",
    "\n",
    "blue_color = '#1f77b4'\n",
    "orange_color = '#ff7f0e'\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(y=[cls_metrics[\"precision\"]], x=[class_name], name='Precision', marker=dict(color=blue_color)))\n",
    "fig.add_trace(go.Bar(y=[cls_metrics[\"recall\"]], x=[class_name], name='Recall', marker=dict(color=orange_color)))\n",
    "fig.update_layout(barmode='group', title=\"Precision and Recall: \" + class_name)\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Value\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve per-class\n",
    "df = pd.DataFrame(pr_curve, columns=m.cat_names)[[class_name]]\n",
    "\n",
    "fig = px.line(df, x=m.recThrs, y=df.columns, title=\"Precision-Recall Curve: \"+ class_name,\n",
    "              labels={\"x\": \"Recall\", \"value\": \"Precision\", \"variable\": \"Category\"},\n",
    "              color_discrete_sequence=px.colors.qualitative.Prism, width=800, height=600)\n",
    "\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.update_xaxes(range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nz = np.nonzero(confusion_matrix[class_idx, :-1])[0]\n",
    "x_nz = np.nonzero(confusion_matrix[:-1, class_idx])[0]\n",
    "idxs = np.union1d(y_nz, x_nz)\n",
    "if class_idx not in idxs:\n",
    "    idxs = np.concatenate([idxs, [class_idx]])\n",
    "idxs = np.sort(idxs)\n",
    "\n",
    "# get confusion matrix for the selected classes\n",
    "confusion_matrix_mini = confusion_matrix[idxs][:, idxs].copy()\n",
    "self_idx = idxs == class_idx\n",
    "v = confusion_matrix_mini[self_idx, self_idx]\n",
    "confusion_matrix_mini[np.diag_indices_from(confusion_matrix_mini)] *= 0\n",
    "confusion_matrix_mini[self_idx, self_idx] = v\n",
    "\n",
    "cat_names_cls = [m.cat_names[i] for i in idxs]\n",
    "confusion_matrix_df_mini = pd.DataFrame(np.log(confusion_matrix_mini), index=cat_names_cls, columns=cat_names_cls)\n",
    "fig = px.imshow(confusion_matrix_df_mini, labels=dict(x=\"Ground Truth\", y=\"Predicted\", color=\"Count\"),\n",
    "                title=f\"Confusion Matrix: {class_name} (log-scale)\")\n",
    "                # width=1000, height=1000)\n",
    "\n",
    "# Hover text\n",
    "fig.update_traces(customdata=confusion_matrix_mini,\n",
    "                  hovertemplate='Count: %{customdata}<br>Predicted: %{y}<br>Ground Truth: %{x}<extra></extra>')\n",
    "\n",
    "# Text on cells\n",
    "if len(idxs) <= 20:\n",
    "    fig.update_traces(text=confusion_matrix_mini,\n",
    "                      texttemplate=\"%{text}\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: avoid \"class_idx + 1\"\n",
    "scores_tp = [m['score'] for m in m.tp_matches if m['category_id'] == class_idx + 1]\n",
    "scores_fp = [m['score'] for m in m.fp_matches if m['category_id'] == class_idx + 1]\n",
    "\n",
    "n_bins = 10\n",
    "xbins_size = 1/n_bins\n",
    "tp_y, tp_x = np.histogram(scores_tp, bins=n_bins, range=[0,1])\n",
    "fp_y, fp_x = np.histogram(scores_fp, bins=n_bins, range=[0,1])\n",
    "dx = (tp_x[1] - tp_x[0])/2\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=scores_tp, name='TP', marker=dict(color='#1fb466'), opacity=0.5, xbins=dict(size=xbins_size)))\n",
    "fig.add_trace(go.Histogram(x=scores_fp, name='FP', marker=dict(color='#dd3f3f'), opacity=0.5, xbins=dict(size=xbins_size)))\n",
    "# fig.add_trace(go.Scatter(x=tp_x+dx, y=tp_y, mode='lines', name='TP', line=dict(color='#1fb466', width=2)))\n",
    "# fig.add_trace(go.Scatter(x=fp_x+dx, y=fp_y, mode='lines', name='FP', line=dict(color='#dd3f3f', width=2)))\n",
    "\n",
    "# Best threshold\n",
    "fig.add_shape(type=\"line\", x0=f1_optimal_conf, x1=f1_optimal_conf, y0=0, y1=tp_y.max()*1.3, line=dict(color=\"orange\", width=1, dash=\"dash\"))\n",
    "fig.add_annotation(x=f1_optimal_conf, y=tp_y.max()*1.3, text=f\"F1-optimal threshold: {f1_optimal_conf:.2f}\", showarrow=False)\n",
    "\n",
    "fig.update_layout(barmode='overlay', title=\"Histogram of Confidence Scores (TP vs FP)\",\n",
    "                  width=800, height=500)\n",
    "fig.update_xaxes(title_text=\"Confidence Score\", range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"Count\", range=[0, tp_y.max()*1.3])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "t = 0\n",
    "tp = m.true_positives.sum(0)[:,t]\n",
    "fp = m.false_positives.sum(0)[:,t]\n",
    "fn = m.false_negatives.sum(0)[:,t]\n",
    "\n",
    "y_edges = np.arange(min(tp) - 0.5, max(tp) + 1.5, 1)\n",
    "x_edges = np.arange(min(fp+fn) - 0.5, max(fp+fn) + 1.5, 1)\n",
    "heatmap, y_edges, x_edges = np.histogram2d(tp, fp+fn, bins=(y_edges, x_edges))\n",
    "\n",
    "z_max = np.max(heatmap)\n",
    "gamma = 0.95\n",
    "\n",
    "colors = np.zeros((heatmap.shape[0], heatmap.shape[1], 3))  # for RGB channels\n",
    "colormap_name = 'RdYlGn_r'\n",
    "cmap = cm.get_cmap(colormap_name)\n",
    "\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        tp_val = x_edges[j] + 0.5\n",
    "        fp_fn_val = y_edges[i] + 0.5\n",
    "        \n",
    "        intensity = heatmap[i, j]\n",
    "        if tp_val + fp_fn_val > 0:\n",
    "            value = tp_val / (tp_val + fp_fn_val)\n",
    "        else:\n",
    "            value = 0\n",
    "        \n",
    "        # green to red colormap\n",
    "        colormap_name = 'RdYlGn_r'\n",
    "        color = cmap(value)  # Get a color from a colormap\n",
    "        # Adjust the color intensity based on the heatmap value\n",
    "        if intensity > 0:\n",
    "            c = np.array(color[:3]) * max(0.2, np.log(intensity) / np.log(z_max))\n",
    "            colors[i, j, :] = c**gamma\n",
    "        else:\n",
    "            colors[i, j, :] = np.array(color[:3]) * 0.12\n",
    "\n",
    "# Plot the colored heatmap\n",
    "fig = px.imshow(colors, labels=dict(x=\"Count of Errors\", y=\"Count of True Predictions\"), title=\"TP vs FP+FN\", text_auto=True, origin='lower',\n",
    "                width=1000, height=1000)\n",
    "\n",
    "# Adding text to each pixel\n",
    "# for i in range(heatmap.shape[0]):\n",
    "#     for j in range(heatmap.shape[1]):\n",
    "#         fig.add_annotation(\n",
    "#             x=j, \n",
    "#             y=i, \n",
    "#             text=str(int(heatmap[i, j])),\n",
    "#             showarrow=False,\n",
    "#             font=dict(color=\"#ddd\", size=10)\n",
    "#         )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
