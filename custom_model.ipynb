{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval, Params\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "from model_benchmark import metrics, utils\n",
    "from model_benchmark.metric_provider import MetricProvider, METRIC_NAMES\n",
    "from model_benchmark import metric_provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt_path = \"cocoGt.json\"\n",
    "cocoDt_path = \"cocoDt.json\"\n",
    "eval_data_path = \"eval_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt = COCO(cocoGt_path)\n",
    "cocoDt = cocoGt.loadRes(cocoDt_path)\n",
    "# cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "\n",
    "import pickle\n",
    "with open(eval_data_path, 'rb') as f:\n",
    "    eval_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(metric_provider)\n",
    "m = metric_provider.MetricProvider(eval_data, cocoGt, cocoDt)\n",
    "m.base_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Metrics\n",
    "base_metrics = m.base_metrics()\n",
    "r = list(base_metrics.values())\n",
    "theta = [METRIC_NAMES[k] for k in base_metrics.keys()]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=r+[r[0]], theta=theta+[theta[0]], fill='toself', name='Overall Metrics',\n",
    "    hovertemplate='%{theta}: %{r:.2f}<extra></extra>',\n",
    "))\n",
    "fig.update_layout(polar=dict(radialaxis=dict(range=[0., 1.])),\n",
    "                  title=\"Overall Metrics\", width=600, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = m.prediction_table()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome counts\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=[m.TP_count], y=[\"Outcomes\"], name='TP', orientation='h', marker=dict(color='#1fb466')))\n",
    "fig.add_trace(go.Bar(x=[m.FN_count], y=[\"Outcomes\"], name='FN', orientation='h', marker=dict(color='#dd3f3f')))\n",
    "fig.add_trace(go.Bar(x=[m.FP_count], y=[\"Outcomes\"], name='FP', orientation='h', marker=dict(color='#d5a5a5')))\n",
    "fig.update_layout(barmode='stack', title=\"Outcome Counts\")\n",
    "fig.update_xaxes(title_text=\"Count\")\n",
    "# width=600, height=500\n",
    "fig.update_layout(width=600, height=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_metrics_df = m.per_class_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision and Recall bar chart\n",
    "per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"f1\")\n",
    "\n",
    "blue_color = '#1f77b4'\n",
    "orange_color = '#ff7f0e'\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(y=per_class_metrics_df_sorted[\"precision\"], x=per_class_metrics_df_sorted[\"category\"], name='Precision', marker=dict(color=blue_color)))\n",
    "fig.add_trace(go.Bar(y=per_class_metrics_df_sorted[\"recall\"], x=per_class_metrics_df_sorted[\"category\"], name='Recall', marker=dict(color=orange_color)))\n",
    "fig.update_layout(barmode='group', title=\"Per-class Precision and Recall (Sorted by F1)\")\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Value\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision bar chart\n",
    "# per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"precision\")\n",
    "fig = px.bar(per_class_metrics_df_sorted, x='category', y='precision', title=\"Per-class Precision (Sorted by F1)\",\n",
    "             color='precision', color_continuous_scale='Plasma')\n",
    "if len(per_class_metrics_df_sorted) <= 20:\n",
    "    fig.update_traces(text=per_class_metrics_df_sorted[\"precision\"].round(2), textposition='outside')\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Precision\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision bar chart\n",
    "# per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"recall\")\n",
    "fig = px.bar(per_class_metrics_df_sorted, x='category', y='recall', title=\"Per-class Recall (Sorted by F1)\",\n",
    "             color='recall', color_continuous_scale='Plasma')\n",
    "if len(per_class_metrics_df_sorted) <= 20:\n",
    "    fig.update_traces(text=per_class_metrics_df_sorted[\"recall\"].round(2), textposition='outside')\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Recall\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve = m.pr_curve()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=m.coco_params.recThrs, y=pr_curve.mean(-1), mode='lines', name='PR Curve', marker=dict(color='#1f77b4')))\n",
    "fig.update_layout(title=\"Precision-Recall Curve\", xaxis_title=\"Recall\", yaxis_title=\"Precision\")\n",
    "fig.update_traces(fill='tozeroy')\n",
    "fig.update_layout(width=600, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve per-class\n",
    "pr_curve_per_class = m.pr_curve()\n",
    "# shape (n_recall_thresholds, n_classes)\n",
    "df = pd.DataFrame(pr_curve_per_class, columns=m.cat_names)\n",
    "\n",
    "fig = px.line(df, x=df.index, y=df.columns, title=\"Precision-Recall Curve per Class\",\n",
    "              labels={\"index\": \"Recall\", \"value\": \"Precision\", \"variable\": \"Category\"},\n",
    "              color_discrete_sequence=px.colors.qualitative.Prism, width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = m.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cat_names = m.cat_names\n",
    "none_name = \"(None)\"\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(np.log(confusion_matrix), index=cat_names + [none_name], columns=cat_names + [none_name])\n",
    "fig = px.imshow(confusion_matrix_df, labels=dict(x=\"Predicted\", y=\"Ground Truth\", color=\"Count\"), title=\"Confusion Matrix (log-scale)\",\n",
    "                width=1000, height=1000)\n",
    "\n",
    "# Hover text\n",
    "fig.update_traces(customdata=confusion_matrix,\n",
    "                  hovertemplate='Count: %{customdata}<br>Predicted: %{x}<br>Ground Truth: %{y}')\n",
    "\n",
    "# Text on cells\n",
    "if len(cat_names) <= 20:\n",
    "    fig.update_traces(text=confusion_matrix,\n",
    "                      texttemplate=\"%{text}\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently Confused Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of confusion as bar chart\n",
    "confused_df = m.frequently_confused(confusion_matrix, topk_pairs=20)\n",
    "confused_name_pairs = confused_df[\"category_pair\"]\n",
    "confused_prob = confused_df[\"probability\"]\n",
    "x_labels = [f\"{pair[0]} - {pair[1]}\" for pair in confused_name_pairs]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=x_labels, y=confused_prob, marker=dict(color=confused_prob, colorscale=\"Reds\")))\n",
    "fig.update_layout(title=\"Frequently confused class pairs\", xaxis_title=\"Class pair\", yaxis_title=\"Probability\")\n",
    "fig.update_traces(text=confused_prob.round(2))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IoU Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "nbins = 40\n",
    "fig.add_trace(go.Histogram(x=m.ious, nbinsx=nbins))\n",
    "fig.update_layout(title=\"IoU Distribution\", xaxis_title=\"IoU\", yaxis_title=\"Count\",\n",
    "                  width=600, height=500)\n",
    "\n",
    "# Add annotation for mean IoU as vertical line\n",
    "mean_iou = m.ious.mean()\n",
    "y1 = len(m.ious) // nbins\n",
    "fig.add_shape(type=\"line\", x0=mean_iou, x1=mean_iou, y0=0, y1=y1, line=dict(color=\"orange\", width=2, dash=\"dash\"))\n",
    "fig.add_annotation(x=mean_iou, y=y1, text=f\"Mean IoU: {mean_iou:.2f}\", showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curve (only positive predictions)\n",
    "true_probs, pred_probs = m.calibration_metrics.calibration_curve()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pred_probs, y=true_probs, mode='lines+markers', name='Calibration plot (Model)', \n",
    "                         line=dict(color='blue'), marker=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Perfectly calibrated', \n",
    "                         line=dict(color='orange', dash='dash')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Calibration Curve (only positive predictions)',\n",
    "    xaxis_title='Confidence Score',\n",
    "    yaxis_title='Fraction of True Positives',\n",
    "    legend=dict(x=0.6, y=0.1),\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score, Precision, Recall vs Confidence Score\n",
    "scores_vs_metrics = pd.DataFrame(m.calibration_metrics.scores_vs_metrics_avg())\n",
    "idxmax = scores_vs_metrics['f1'].idxmax()\n",
    "best_threshold = scores_vs_metrics['scores'][idxmax]\n",
    "best_f1 = scores_vs_metrics['f1'][idxmax]\n",
    "\n",
    "fig = px.line(df, x=\"scores\", y=[\"precision\", \"recall\", \"f1\"], title=\"Performance at Different Confidence Thresholds\",\n",
    "                labels={\"scores\": \"Confidence Threshold\", \"value\": \"Value\", \"variable\": \"Metric\"},\n",
    "                width=800, height=500)\n",
    "fig.update_layout(yaxis=dict(range=[0, 1]))\n",
    "\n",
    "# Add vertical line for best threshold\n",
    "fig.add_shape(type=\"line\", x0=best_threshold, x1=best_threshold, y0=0, y1=best_f1, line=dict(color=\"orange\", width=2, dash=\"dash\"))\n",
    "fig.add_annotation(x=best_threshold, y=best_f1+0.04, text=f\"F1-optimal threshold: {best_threshold:.2f}\", showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of confidence scores (TP vs FP)\n",
    "scores_tp, scores_fp = m.calibration_metrics.scores_tp_and_fp(iou_idx=0)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=scores_tp, name='TP', marker=dict(color='#1fb466'), opacity=0.7))\n",
    "fig.add_trace(go.Histogram(x=scores_fp, name='FP', marker=dict(color='#dd3f3f'), opacity=0.7))\n",
    "fig.update_layout(barmode='overlay', title=\"Histogram of Confidence Scores (TP vs FP)\",\n",
    "                  width=800, height=500)\n",
    "fig.update_xaxes(title_text=\"Confidence Score\", range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP per-class\n",
    "ap_per_class = m.coco_precision[:, :, :, 0, 2].mean(axis=(0, 1))\n",
    "# Per-class Average Precision (AP)\n",
    "fig = px.scatter_polar(r=ap_per_class, theta=m.cat_names, title=\"Per-class Average Precision (AP)\",\n",
    "                       labels=dict(r=\"Average Precision\", theta=\"Category\"),\n",
    "                       width=800, height=800,\n",
    "                       range_r=[0, 1])\n",
    "# fill points\n",
    "fig.update_traces(fill='toself')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Counts\n",
    "iou_thres = 0\n",
    "\n",
    "tp = m.true_positives.sum(1)[:,iou_thres]\n",
    "fp = m.false_positives.sum(1)[:,iou_thres]\n",
    "fn = m.false_negatives.sum(1)[:,iou_thres]\n",
    "\n",
    "# normalize\n",
    "support = tp + fn\n",
    "tp_rel = tp / support\n",
    "fp_rel = fp / support\n",
    "fn_rel = fn / support\n",
    "\n",
    "# sort by f1\n",
    "sort_scores = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "K = len(m.cat_names)\n",
    "sort_indices = np.argsort(sort_scores)\n",
    "cat_names_sorted = [m.cat_names[i] for i in sort_indices]\n",
    "tp_rel, fn_rel, fp_rel = tp_rel[sort_indices], fn_rel[sort_indices], fp_rel[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked per-class counts\n",
    "data = {\n",
    "    \"count\": np.concatenate([tp_rel, fn_rel, fp_rel]),\n",
    "    \"type\": [\"TP\"]*K + [\"FN\"]*K + [\"FP\"]*K,\n",
    "    \"category\": cat_names_sorted*3\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "color_map = {\n",
    "    'TP': '#1fb466',\n",
    "    'FN': '#dd3f3f',\n",
    "    'FP': '#d5a5a5'\n",
    "}\n",
    "fig = px.bar(df, x=\"category\", y=\"count\", color=\"type\", title=\"Per-class Outcome Counts\",\n",
    "             labels={'count': 'Total Count', \"category\": \"Category\"},\n",
    "             color_discrete_map=color_map)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "t = 0\n",
    "tp = m.true_positives.sum(0)[:,t]\n",
    "fp = m.false_positives.sum(0)[:,t]\n",
    "fn = m.false_negatives.sum(0)[:,t]\n",
    "\n",
    "y_edges = np.arange(min(tp) - 0.5, max(tp) + 1.5, 1)\n",
    "x_edges = np.arange(min(fp+fn) - 0.5, max(fp+fn) + 1.5, 1)\n",
    "heatmap, y_edges, x_edges = np.histogram2d(tp, fp+fn, bins=(y_edges, x_edges))\n",
    "\n",
    "z_max = np.max(heatmap)\n",
    "gamma = 0.95\n",
    "\n",
    "colors = np.zeros((heatmap.shape[0], heatmap.shape[1], 3))  # for RGB channels\n",
    "colormap_name = 'RdYlGn_r'\n",
    "cmap = cm.get_cmap(colormap_name)\n",
    "\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        tp_val = x_edges[j] + 0.5\n",
    "        fp_fn_val = y_edges[i] + 0.5\n",
    "        \n",
    "        intensity = heatmap[i, j]\n",
    "        if tp_val + fp_fn_val > 0:\n",
    "            value = tp_val / (tp_val + fp_fn_val)\n",
    "        else:\n",
    "            value = 0\n",
    "        \n",
    "        # green to red colormap\n",
    "        colormap_name = 'RdYlGn_r'\n",
    "        color = cmap(value)  # Get a color from a colormap\n",
    "        # Adjust the color intensity based on the heatmap value\n",
    "        if intensity > 0:\n",
    "            c = np.array(color[:3]) * max(0.2, np.log(intensity) / np.log(z_max))\n",
    "            colors[i, j, :] = c**gamma\n",
    "        else:\n",
    "            colors[i, j, :] = np.array(color[:3]) * 0.12\n",
    "\n",
    "# Plot the colored heatmap\n",
    "fig = px.imshow(colors, labels=dict(x=\"Count of Errors\", y=\"Count of True Predictions\"), title=\"TP vs FP+FN\", text_auto=True, origin='lower',\n",
    "                width=1000, height=1000)\n",
    "\n",
    "# Adding text to each pixel\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        fig.add_annotation(\n",
    "            x=j, \n",
    "            y=i, \n",
    "            text=str(int(heatmap[i, j])),\n",
    "            showarrow=False,\n",
    "            font=dict(color=\"#ddd\", size=10)\n",
    "        )\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
