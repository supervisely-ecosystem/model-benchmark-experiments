{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval, Params\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "from model_benchmark import metrics, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt = COCO(\"cocoGt.json\")\n",
    "cocoDt = cocoGt.loadRes(\"cocoDt.json\")\n",
    "# cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "\n",
    "import pickle\n",
    "with open('eval_data.pkl', 'rb') as f:\n",
    "    eval_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import model_benchmark.metrics as metrics\n",
    "\n",
    "\n",
    "def get_outcomes_per_image(matches, cocoGt: COCO):\n",
    "    img_ids = sorted(cocoGt.getImgIds())\n",
    "    imgId2idx = {img_id: idx for idx, img_id in enumerate(img_ids)}\n",
    "    outcomes_per_image = np.zeros((len(img_ids), 3), dtype=float)\n",
    "    for m in matches:\n",
    "        img_id = m[\"image_id\"]\n",
    "        idx = imgId2idx[img_id]\n",
    "        if m[\"type\"] == \"TP\":\n",
    "            outcomes_per_image[idx, 0] += 1\n",
    "        elif m[\"type\"] == \"FP\":\n",
    "            outcomes_per_image[idx, 1] += 1\n",
    "        elif m[\"type\"] == \"FN\":\n",
    "            outcomes_per_image[idx, 2] += 1\n",
    "    return img_ids, outcomes_per_image\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, eval_data: dict, cocoGt: COCO, cocoDt: COCO):\n",
    "\n",
    "        # eval_data\n",
    "        self.true_positives = eval_data[\"true_positives\"]\n",
    "        self.false_positives = eval_data[\"false_positives\"]\n",
    "        self.false_negatives = eval_data[\"false_negatives\"]\n",
    "        self.matches = eval_data[\"matches\"]\n",
    "        self.coco_stats = eval_data[\"coco_stats\"]\n",
    "        self.coco_precision = eval_data[\"coco_precision\"]\n",
    "        self.coco_params : Params = eval_data[\"coco_params\"]\n",
    "\n",
    "        # Counts\n",
    "        self.TP_count = int(self.true_positives[...,0].sum())\n",
    "        self.FP_count = int(self.false_positives[...,0].sum())\n",
    "        self.FN_count = int(self.false_negatives[...,0].sum())\n",
    "\n",
    "        # Matches\n",
    "        self.tp_matches = [m for m in self.matches if m['type'] == \"TP\"]\n",
    "        self.fp_matches = [m for m in self.matches if m['type'] == \"FP\"]\n",
    "        self.fn_matches = [m for m in self.matches if m['type'] == \"FN\"]\n",
    "        self.confused_matches = [m for m in self.fp_matches if m['miss_cls']]\n",
    "        self.fp_not_confused_matches = [m for m in self.fp_matches if not m['miss_cls']]\n",
    "        self.ious = np.array([m['iou'] for m in self.matches if m['iou']])\n",
    "\n",
    "        # Calibration\n",
    "        self.calibration_metrics = CalibrationMetrics(self.tp_matches, self.fp_matches, self.fn_matches, self.coco_params.iouThrs)\n",
    "\n",
    "        # info\n",
    "        self.cat_ids = cocoGt.getCatIds()\n",
    "        self.cat_names = [cocoGt.cats[cat_id]['name'] for cat_id in self.cat_ids]\n",
    "\n",
    "    def base_metrics(self):\n",
    "        tp = self.true_positives.sum(1)\n",
    "        fp = self.false_positives.sum(1)\n",
    "        fn = self.false_negatives.sum(1)\n",
    "        confuse_count = len(self.confused_matches)\n",
    "\n",
    "        mAP = self.coco_stats[0]\n",
    "        precision = np.mean(tp / (tp + fp))\n",
    "        recall = np.mean(tp / (tp + fn))\n",
    "        iou = np.mean(self.ious)\n",
    "        classification_accuracy = self.TP_count / (self.TP_count + confuse_count)\n",
    "        calibration_score = 1 - self.calibration_metrics.maximum_calibration_error()\n",
    "\n",
    "        return {\n",
    "            \"mAP\": mAP,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"iou\": iou,\n",
    "            \"classification_accuracy\": classification_accuracy,\n",
    "            \"calibration_score\": calibration_score\n",
    "        }\n",
    "    \n",
    "    def per_class_metrics(self):\n",
    "        tp = self.true_positives.sum(1).mean(1)\n",
    "        fp = self.false_positives.sum(1).mean(1)\n",
    "        fn = self.false_negatives.sum(1).mean(1)\n",
    "        pr = tp / (tp + fp)\n",
    "        rc = tp / (tp + fn)\n",
    "        f1 = 2 * pr * rc / (pr + rc)\n",
    "        return pd.DataFrame({\n",
    "            \"category\": self.cat_names,\n",
    "            \"precision\": pr,\n",
    "            \"recall\": rc,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "    \n",
    "    def pr_curve(self):\n",
    "        pr_curve = self.coco_precision[:,:,:,0,2].mean(0)\n",
    "        return pr_curve\n",
    "    \n",
    "    def prediction_table(self):\n",
    "        img_ids, outcomes_per_image = get_outcomes_per_image(self.matches, cocoGt)\n",
    "        image_names = [cocoGt.imgs[img_id][\"file_name\"] for img_id in img_ids]\n",
    "        # inference_time = ...\n",
    "        n_gt = outcomes_per_image[:,0] + outcomes_per_image[:,2]\n",
    "        n_dt = outcomes_per_image[:,0] + outcomes_per_image[:,1]\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            precision_per_image = outcomes_per_image[:,0] / n_dt\n",
    "            recall_per_image = outcomes_per_image[:,0] / n_gt\n",
    "            f1_per_image = 2 * precision_per_image * recall_per_image / (precision_per_image + recall_per_image)\n",
    "        prediction_table = pd.DataFrame({\n",
    "            \"image_name\": image_names,\n",
    "            \"N gt\": n_gt,\n",
    "            \"N dt\": n_dt,\n",
    "            \"TP\": outcomes_per_image[:,0],\n",
    "            \"FP\": outcomes_per_image[:,1],\n",
    "            \"FN\": outcomes_per_image[:,2],\n",
    "            \"Precision\": precision_per_image,\n",
    "            \"Recall\": recall_per_image,\n",
    "            \"F1\": f1_per_image\n",
    "            })\n",
    "        return prediction_table\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        K = len(self.cat_ids)\n",
    "        catId2idx = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\n",
    "        idx2catId = {i: cat_id for cat_id, i in catId2idx.items()}\n",
    "\n",
    "        confusion_matrix = np.zeros((K+1, K+1), dtype=int)\n",
    "\n",
    "        for m in self.confused_matches:\n",
    "            cat_idx_pred = catId2idx[m['category_id']]\n",
    "            cat_idx_gt = catId2idx[cocoGt.anns[m['gt_id']]['category_id']]\n",
    "            confusion_matrix[cat_idx_pred, cat_idx_gt] += 1\n",
    "\n",
    "        for m in self.tp_matches:\n",
    "            cat_idx = catId2idx[m['category_id']]\n",
    "            confusion_matrix[cat_idx, cat_idx] += 1\n",
    "\n",
    "        for m in self.fp_not_confused_matches:\n",
    "            cat_idx_pred = catId2idx[m['category_id']]\n",
    "            confusion_matrix[cat_idx_pred, -1] += 1\n",
    "\n",
    "        for m in self.fn_matches:\n",
    "            cat_idx_gt = catId2idx[m['category_id']]\n",
    "            confusion_matrix[-1, cat_idx_gt] += 1\n",
    "        \n",
    "        return confusion_matrix\n",
    "    \n",
    "    def frequently_confused(self, confusion_matrix, topk_pairs=20):\n",
    "        # Frequently confused class pairs\n",
    "        idx2catId = {i: cat_id for i, cat_id in enumerate(self.cat_ids)}\n",
    "        cm = confusion_matrix[:-1,:-1]\n",
    "        cm_l = np.tril(cm, -1)\n",
    "        cm_u = np.triu(cm, 1)\n",
    "        cm = cm_l + cm_u.T\n",
    "        cm_flat = cm.flatten()\n",
    "        inds_sort = np.argsort(-cm_flat)[:topk_pairs]\n",
    "        inds_sort = inds_sort[cm_flat[inds_sort] > 0]  # remove zeros\n",
    "        inds_sort = np.unravel_index(inds_sort, cm.shape)\n",
    "\n",
    "        # probability of confusion: (predicted A, actually B + predicted B, actually A) / (predicted A + predicted B)\n",
    "        confused_counts = cm[inds_sort]\n",
    "        dt_total = confusion_matrix.sum(1)\n",
    "        dt_pair_sum = np.array([dt_total[i] + dt_total[j] for i, j in zip(*inds_sort)])\n",
    "        confused_prob = confused_counts / dt_pair_sum\n",
    "        inds_sort2 = np.argsort(-confused_prob)\n",
    "\n",
    "        confused_idxs = np.array(inds_sort).T[inds_sort2]\n",
    "        confused_name_pairs = [(self.cat_names[i], self.cat_names[j]) for i, j in confused_idxs]\n",
    "        confused_counts = confused_counts[inds_sort2]\n",
    "        confused_prob = confused_prob[inds_sort2]\n",
    "        confused_catIds = [(idx2catId[i], idx2catId[j]) for i, j in confused_idxs]\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"category_pair\": confused_name_pairs,\n",
    "            \"category_id_pair\": confused_catIds,\n",
    "            \"count\": confused_counts,\n",
    "            \"probability\": confused_prob\n",
    "        })\n",
    "\n",
    "    def iou_histogram(self):\n",
    "        iou_hist = np.histogram(self.ious, range=(0.5, 1))\n",
    "        return iou_hist\n",
    "    \n",
    "    \n",
    "class CalibrationMetrics:\n",
    "    def __init__(self, tp_matches, fp_matches, fn_matches, iouThrs):\n",
    "        scores = []\n",
    "        classes = []\n",
    "        iou_idxs = []\n",
    "        p_matches = tp_matches + fp_matches\n",
    "        per_class_count = defaultdict(int)\n",
    "        for m in p_matches:\n",
    "            if m['type'] == \"TP\" and m['iou'] is not None:\n",
    "                iou_idx = np.searchsorted(iouThrs, m['iou'])\n",
    "                iou_idxs.append(iou_idx)\n",
    "                assert iou_idx > 0\n",
    "            else:\n",
    "                iou_idxs.append(0)\n",
    "            scores.append(m['score'])\n",
    "            classes.append(m[\"category_id\"])\n",
    "            if m['type'] == \"TP\":\n",
    "                per_class_count[m[\"category_id\"]] += 1\n",
    "        for m in fn_matches:\n",
    "            per_class_count[m[\"category_id\"]] += 1\n",
    "        per_class_count = dict(per_class_count)\n",
    "        scores = np.array(scores)\n",
    "        inds_sort = np.argsort(-scores)\n",
    "        scores = scores[inds_sort]\n",
    "        classes = np.array(classes)[inds_sort]\n",
    "        iou_idxs = np.array(iou_idxs)[inds_sort]\n",
    "\n",
    "        self.scores = scores\n",
    "        self.classes = classes\n",
    "        self.iou_idxs = iou_idxs\n",
    "        self.per_class_count = per_class_count\n",
    "\n",
    "        # TODO What does it mean: self.iou_idxs > iou_idx\n",
    "        self.y_true = self.iou_idxs > iou_idx\n",
    "\n",
    "    def scores_vs_metrics(self, iou_idx=0, cat_id=None):\n",
    "        tps = self.iou_idxs > iou_idx\n",
    "        if cat_id is not None:\n",
    "            cls_mask = self.classes == cat_id\n",
    "            tps = tps[cls_mask]\n",
    "            scores = self.scores[cls_mask]\n",
    "            n_positives = self.per_class_count[cat_id]\n",
    "        else:\n",
    "            scores = self.scores\n",
    "            n_positives = sum(self.per_class_count.values())\n",
    "        fps = ~tps\n",
    "\n",
    "        tps_sum = tps.cumsum()\n",
    "        fps_sum = fps.cumsum()\n",
    "\n",
    "        # Precision, recall, f1\n",
    "        precision = tps_sum / (tps_sum + fps_sum)\n",
    "        recall = tps_sum / n_positives\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    def calibration_curve(self):\n",
    "        true_probs, pred_probs = calibration_curve(self.y_true, self.scores, n_bins=10)\n",
    "        return true_probs, pred_probs\n",
    "    \n",
    "    def maximum_calibration_error(self):\n",
    "        return metrics.maximum_calibration_error(self.y_true, self.scores, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Metrics(eval_data, cocoGt, cocoDt)\n",
    "m.base_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = {\n",
    "    \"mAP\": \"mAP\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"recall\": \"Recall\",\n",
    "    \"iou\": \"IoU\",\n",
    "    \"classification_accuracy\": \"Classification Accuracy\",\n",
    "    \"calibration_score\": \"Calibration Score\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Radar Chart\n",
    "base_metrics = m.base_metrics()\n",
    "r = list(base_metrics.values())\n",
    "theta = [metric_names[k] for k in base_metrics.keys()]\n",
    "fig = px.line_polar(\n",
    "    r=r, theta=theta, line_close=True,\n",
    "    title=\"Overall Metrics\", width=600, height=500\n",
    "    )\n",
    "fig.update_traces(fill='toself')\n",
    "fig.update_layout(polar=dict(radialaxis=dict(range=[0., 1.])))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = m.prediction_table()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome counts\n",
    "TP_count, FN_count, FP_count = m.TP_count, m.FN_count, m.FP_count\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=[TP_count], y=[\"Outcomes\"], name='TP', orientation='h', marker=dict(color='#1fb466')))\n",
    "fig.add_trace(go.Bar(x=[FN_count], y=[\"Outcomes\"], name='FN', orientation='h', marker=dict(color='#dd3f3f')))\n",
    "fig.add_trace(go.Bar(x=[FP_count], y=[\"Outcomes\"], name='FP', orientation='h', marker=dict(color='#d5a5a5')))\n",
    "fig.update_layout(barmode='stack', title=\"Outcome Counts\")\n",
    "fig.update_xaxes(title_text=\"Count\")\n",
    "# width=600, height=500\n",
    "fig.update_layout(width=600, height=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_metrics_df = m.per_class_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision and Recall bar chart\n",
    "per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"f1\")\n",
    "\n",
    "blue_color = '#1f77b4'\n",
    "orange_color = '#ff7f0e'\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(y=per_class_metrics_df_sorted[\"precision\"], x=per_class_metrics_df_sorted[\"category\"], name='Precision', marker=dict(color=blue_color)))\n",
    "fig.add_trace(go.Bar(y=per_class_metrics_df_sorted[\"recall\"], x=per_class_metrics_df_sorted[\"category\"], name='Recall', marker=dict(color=orange_color)))\n",
    "fig.update_layout(barmode='group', title=\"Per-class Precision and Recall (Sorted by F1)\")\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Value\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision bar chart\n",
    "# per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"precision\")\n",
    "fig = px.bar(per_class_metrics_df_sorted, x='category', y='precision', title=\"Per-class Precision (Sorted by F1)\",\n",
    "             color='precision', color_continuous_scale='Plasma')\n",
    "if len(per_class_metrics_df_sorted) <= 20:\n",
    "    fig.update_traces(text=per_class_metrics_df_sorted[\"precision\"].round(2), textposition='outside')\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Precision\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Precision bar chart\n",
    "# per_class_metrics_df_sorted = per_class_metrics_df.sort_values(by=\"recall\")\n",
    "fig = px.bar(per_class_metrics_df_sorted, x='category', y='recall', title=\"Per-class Recall (Sorted by F1)\",\n",
    "             color='recall', color_continuous_scale='Plasma')\n",
    "if len(per_class_metrics_df_sorted) <= 20:\n",
    "    fig.update_traces(text=per_class_metrics_df_sorted[\"recall\"].round(2), textposition='outside')\n",
    "fig.update_xaxes(title_text=\"Category\")\n",
    "fig.update_yaxes(title_text=\"Recall\", range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve = m.pr_curve()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=m.coco_params.recThrs, y=pr_curve.mean(-1), mode='lines', name='PR Curve', marker=dict(color='#1f77b4')))\n",
    "fig.update_layout(title=\"Precision-Recall Curve\", xaxis_title=\"Recall\", yaxis_title=\"Precision\")\n",
    "fig.update_traces(fill='tozeroy')\n",
    "fig.update_layout(width=600, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve per-class\n",
    "pr_curve_per_class = m.pr_curve()\n",
    "# shape (n_recall_thresholds, n_classes)\n",
    "df = pd.DataFrame(pr_curve_per_class, columns=m.cat_names)\n",
    "\n",
    "fig = px.line(df, x=df.index, y=df.columns, title=\"Precision-Recall Curve per Class\",\n",
    "              labels={\"index\": \"Recall\", \"value\": \"Precision\", \"variable\": \"Category\"},\n",
    "              color_discrete_sequence=px.colors.qualitative.Prism, width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = m.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cat_names = m.cat_names\n",
    "none_name = \"(None)\"\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(np.log(confusion_matrix), index=cat_names + [none_name], columns=cat_names + [none_name])\n",
    "fig = px.imshow(confusion_matrix_df, labels=dict(x=\"Predicted\", y=\"Ground Truth\", color=\"Count\"), title=\"Confusion Matrix (log-scale)\",\n",
    "                width=1000, height=1000)\n",
    "\n",
    "# Hover text\n",
    "fig.update_traces(customdata=confusion_matrix,\n",
    "                  hovertemplate='Count: %{customdata}<br>Predicted: %{x}<br>Ground Truth: %{y}')\n",
    "\n",
    "# Text on cells\n",
    "if len(cat_names) <= 20:\n",
    "    fig.update_traces(text=confusion_matrix,\n",
    "                      texttemplate=\"%{text}\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently Confused Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of confusion as bar chart\n",
    "confused_df = m.frequently_confused(confusion_matrix, topk_pairs=20)\n",
    "confused_name_pairs = confused_df[\"category_pair\"]\n",
    "confused_prob = confused_df[\"probability\"]\n",
    "x_labels = [f\"{pair[0]} - {pair[1]}\" for pair in confused_name_pairs]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=x_labels, y=confused_prob, marker=dict(color=confused_prob, colorscale=\"Reds\")))\n",
    "fig.update_layout(title=\"Frequently confused class pairs\", xaxis_title=\"Class pair\", yaxis_title=\"Probability\")\n",
    "fig.update_traces(text=confused_prob.round(2))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IoU Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_hist = m.iou_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=iou_hist[1], y=iou_hist[0]))\n",
    "fig.update_layout(title=\"IoU Distribution\", xaxis_title=\"IoU\", yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_probs, pred_probs = m.calibration_metrics.calibration_curve()\n",
    "# Ð¡alibration curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(pred_probs, true_probs, marker='o', linewidth=1, label='Calibration plot (Model)')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Fraction of True Positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_vs_metrics = pd.DataFrame(m.calibration_metrics.scores_vs_metrics())\n",
    "# F1 score, Precision, Recall vs Confidence Score\n",
    "\n",
    "fig = px.line(scores_vs_metrics, x=\"scores\", y=[\"precision\", \"recall\", \"f1\"], title=\"F1 score, Precision, Recall vs Confidence Score\",\n",
    "                labels={\"scores\": \"Confidence Score\", \"value\": \"Value\", \"variable\": \"Metric\"})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps = m.calibration_metrics.iou_idxs > 0\n",
    "scores = m.calibration_metrics.scores\n",
    "scores_tp = scores[tps]\n",
    "scores_fp = scores[~tps]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=scores_tp, name='TP', marker=dict(color='#1fb466'), opacity=0.7))\n",
    "fig.add_trace(go.Histogram(x=scores_fp, name='FP', marker=dict(color='#dd3f3f'), opacity=0.7))\n",
    "fig.update_layout(barmode='overlay', title=\"Histogram of Confidence Scores (TP vs FP)\")\n",
    "fig.update_xaxes(title_text=\"Confidence Score\")\n",
    "fig.update_yaxes(title_text=\"Density\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP per-class\n",
    "ap_per_class = m.coco_precision[:, :, :, 0, 2].mean(axis=(0, 1))\n",
    "# Per-class Average Precision (AP)\n",
    "fig = px.scatter_polar(r=ap_per_class, theta=m.cat_names, title=\"Per-class Average Precision (AP)\",\n",
    "                       labels=dict(r=\"Average Precision\", theta=\"Category\"),\n",
    "                       width=800, height=800,\n",
    "                       range_r=[0, 1])\n",
    "# fill points\n",
    "fig.update_traces(fill='toself')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Counts\n",
    "iou_thres = 0\n",
    "\n",
    "tp = m.true_positives.sum(1)[:,iou_thres]\n",
    "fp = m.false_positives.sum(1)[:,iou_thres]\n",
    "fn = m.false_negatives.sum(1)[:,iou_thres]\n",
    "\n",
    "# normalize\n",
    "support = tp + fn\n",
    "tp_rel = tp / support\n",
    "fp_rel = fp / support\n",
    "fn_rel = fn / support\n",
    "\n",
    "# sort by f1\n",
    "sort_scores = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "K = len(m.cat_names)\n",
    "sort_indices = np.argsort(sort_scores)\n",
    "cat_names_sorted = [m.cat_names[i] for i in sort_indices]\n",
    "tp_rel, fn_rel, fp_rel = tp_rel[sort_indices], fn_rel[sort_indices], fp_rel[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked per-class counts\n",
    "data = {\n",
    "    \"count\": np.concatenate([tp_rel, fn_rel, fp_rel]),\n",
    "    \"type\": [\"TP\"]*K + [\"FN\"]*K + [\"FP\"]*K,\n",
    "    \"category\": cat_names_sorted*3\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "color_map = {\n",
    "    'TP': '#1fb466',\n",
    "    'FN': '#dd3f3f',\n",
    "    'FP': '#d5a5a5'\n",
    "}\n",
    "fig = px.bar(df, x=\"category\", y=\"count\", color=\"type\", title=\"Per-class Outcome Counts\",\n",
    "             labels={'count': 'Total Count', \"category\": \"Category\"},\n",
    "             color_discrete_map=color_map)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "t = 0\n",
    "tp = m.true_positives.sum(0)[:,t]\n",
    "fp = m.false_positives.sum(0)[:,t]\n",
    "fn = m.false_negatives.sum(0)[:,t]\n",
    "\n",
    "y_edges = np.arange(min(tp) - 0.5, max(tp) + 1.5, 1)\n",
    "x_edges = np.arange(min(fp+fn) - 0.5, max(fp+fn) + 1.5, 1)\n",
    "heatmap, y_edges, x_edges = np.histogram2d(tp, fp+fn, bins=(y_edges, x_edges))\n",
    "\n",
    "z_max = np.max(heatmap)\n",
    "gamma = 0.95\n",
    "\n",
    "colors = np.zeros((heatmap.shape[0], heatmap.shape[1], 3))  # for RGB channels\n",
    "colormap_name = 'RdYlGn_r'\n",
    "cmap = cm.get_cmap(colormap_name)\n",
    "\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        tp_val = x_edges[j] + 0.5\n",
    "        fp_fn_val = y_edges[i] + 0.5\n",
    "        \n",
    "        intensity = heatmap[i, j]\n",
    "        if tp_val + fp_fn_val > 0:\n",
    "            value = tp_val / (tp_val + fp_fn_val)\n",
    "        else:\n",
    "            value = 0\n",
    "        \n",
    "        # green to red colormap\n",
    "        colormap_name = 'RdYlGn_r'\n",
    "        color = cmap(value)  # Get a color from a colormap\n",
    "        # Adjust the color intensity based on the heatmap value\n",
    "        if intensity > 0:\n",
    "            c = np.array(color[:3]) * max(0.2, np.log(intensity) / np.log(z_max))\n",
    "            colors[i, j, :] = c**gamma\n",
    "        else:\n",
    "            colors[i, j, :] = np.array(color[:3]) * 0.12\n",
    "\n",
    "# Plot the colored heatmap\n",
    "fig = px.imshow(colors, labels=dict(x=\"Count of Errors\", y=\"Count of True Predictions\"), title=\"TP vs FP+FN\", text_auto=True, origin='lower',\n",
    "                width=1000, height=1000)\n",
    "\n",
    "# Adding text to each pixel\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        fig.add_annotation(\n",
    "            x=j, \n",
    "            y=i, \n",
    "            text=str(int(heatmap[i, j])),\n",
    "            showarrow=False,\n",
    "            font=dict(color=\"#ddd\", size=10)\n",
    "        )\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
