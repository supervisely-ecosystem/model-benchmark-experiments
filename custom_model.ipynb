{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval, Params\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "from model_benchmark import metrics, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt = COCO(\"cocoGt.json\")\n",
    "cocoDt = cocoGt.loadRes(\"cocoDt.json\")\n",
    "# cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "\n",
    "import pickle\n",
    "with open('eval_data.pkl', 'rb') as f:\n",
    "    eval_data = pickle.load(f)\n",
    "\n",
    "# true_positives = eval_data[\"true_positives\"]\n",
    "# false_positives = eval_data[\"false_positives\"]\n",
    "# false_negatives = eval_data[\"false_negatives\"]\n",
    "# matches = eval_data[\"matches\"]\n",
    "# coco_stats = eval_data[\"coco_stats\"]\n",
    "# coco_precision = eval_data[\"coco_precision\"]\n",
    "\n",
    "# # basic calculation\n",
    "# TP_count = int(true_positives[...,0].sum())\n",
    "# FP_count = int(false_positives[...,0].sum())\n",
    "# FN_count = int(false_negatives[...,0].sum())\n",
    "\n",
    "# tp_matches = [m for m in matches if m['type'] == \"TP\"]\n",
    "# fp_matches = [m for m in matches if m['type'] == \"FP\"]\n",
    "# fn_matches = [m for m in matches if m['type'] == \"FN\"]\n",
    "# confused_matches = [m for m in fp_matches if m['miss_cls']]\n",
    "# fp_not_confused_matches = [m for m in fp_matches if not m['miss_cls']]\n",
    "# ious = np.array([m['iou'] for m in matches if m['iou']])\n",
    "\n",
    "# # info\n",
    "# cat_ids = cocoGt.getCatIds()\n",
    "# cat_names = [cocoGt.cats[cat_id]['name'] for cat_id in cat_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import model_benchmark.metrics as metrics\n",
    "\n",
    "\n",
    "def get_outcomes_per_image(matches, cocoGt: COCO):\n",
    "    img_ids = sorted(cocoGt.getImgIds())\n",
    "    imgId2idx = {img_id: idx for idx, img_id in enumerate(img_ids)}\n",
    "    outcomes_per_image = np.zeros((len(img_ids), 3), dtype=float)\n",
    "    for m in matches:\n",
    "        img_id = m[\"image_id\"]\n",
    "        idx = imgId2idx[img_id]\n",
    "        if m[\"type\"] == \"TP\":\n",
    "            outcomes_per_image[idx, 0] += 1\n",
    "        elif m[\"type\"] == \"FP\":\n",
    "            outcomes_per_image[idx, 1] += 1\n",
    "        elif m[\"type\"] == \"FN\":\n",
    "            outcomes_per_image[idx, 2] += 1\n",
    "    return img_ids, outcomes_per_image\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, eval_data: dict, cocoGt: COCO, cocoDt: COCO):\n",
    "\n",
    "        # eval_data\n",
    "        self.true_positives = eval_data[\"true_positives\"]\n",
    "        self.false_positives = eval_data[\"false_positives\"]\n",
    "        self.false_negatives = eval_data[\"false_negatives\"]\n",
    "        self.matches = eval_data[\"matches\"]\n",
    "        self.coco_stats = eval_data[\"coco_stats\"]\n",
    "        self.coco_precision = eval_data[\"coco_precision\"]\n",
    "        self.coco_params : Params = eval_data[\"coco_params\"]\n",
    "\n",
    "        # Counts\n",
    "        self.TP_count = int(self.true_positives[...,0].sum())\n",
    "        self.FP_count = int(self.false_positives[...,0].sum())\n",
    "        self.FN_count = int(self.false_negatives[...,0].sum())\n",
    "\n",
    "        # Matches\n",
    "        self.tp_matches = [m for m in self.matches if m['type'] == \"TP\"]\n",
    "        self.fp_matches = [m for m in self.matches if m['type'] == \"FP\"]\n",
    "        self.fn_matches = [m for m in self.matches if m['type'] == \"FN\"]\n",
    "        self.confused_matches = [m for m in self.fp_matches if m['miss_cls']]\n",
    "        self.fp_not_confused_matches = [m for m in self.fp_matches if not m['miss_cls']]\n",
    "        self.ious = np.array([m['iou'] for m in self.matches if m['iou']])\n",
    "\n",
    "        # Calibration\n",
    "        self.calibration_metrics = CalibrationMetrics(self.tp_matches, self.fp_matches, self.fn_matches, self.coco_params.iouThrs)\n",
    "\n",
    "        # info\n",
    "        self.cat_ids = cocoGt.getCatIds()\n",
    "        self.cat_names = [cocoGt.cats[cat_id]['name'] for cat_id in self.cat_ids]\n",
    "\n",
    "    def base_metrics(self):\n",
    "        tp = self.true_positives.sum(1)\n",
    "        fp = self.false_positives.sum(1)\n",
    "        fn = self.false_negatives.sum(1)\n",
    "        confuse_count = len(self.confused_matches)\n",
    "\n",
    "        mAP = self.coco_stats[0]\n",
    "        precision = np.mean(tp / (tp + fp))\n",
    "        recall = np.mean(tp / (tp + fn))\n",
    "        iou = np.mean(self.ious)\n",
    "        classification_accuracy = self.TP_count / (self.TP_count + confuse_count)\n",
    "        calibration_score = 1 - self.calibration_metrics.maximum_calibration_error()\n",
    "\n",
    "        return {\n",
    "            \"mAP\": mAP,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"iou\": iou,\n",
    "            \"classification_accuracy\": classification_accuracy,\n",
    "            \"calibration_score\": calibration_score\n",
    "        }\n",
    "    \n",
    "    def precision_recall_classes(self):\n",
    "        # Per-class metrics\n",
    "        tp = self.true_positives.sum(1).mean(1)\n",
    "        fp = self.false_positives.sum(1).mean(1)\n",
    "        fn = self.false_negatives.sum(1).mean(1)\n",
    "\n",
    "        pr = tp / (tp + fp)\n",
    "        rc = tp / (tp + fn)\n",
    "\n",
    "        pr_sort = np.argsort(pr)\n",
    "        rc_sort = np.argsort(rc)\n",
    "        pr_names = [self.cat_names[i] for i in pr_sort]\n",
    "        rc_names = [self.cat_names[i] for i in rc_sort]\n",
    "        pr_values = pr[pr_sort]\n",
    "        rc_values = rc[rc_sort]\n",
    "        return pr_names, pr_values, rc_names, rc_values\n",
    "    \n",
    "    def pr_curve(self):\n",
    "        pr_curve = self.coco_precision[:,:,:,0,2].mean(0)\n",
    "        return pr_curve\n",
    "    \n",
    "    def prediction_table(self):\n",
    "        img_ids, outcomes_per_image = get_outcomes_per_image(self.matches, cocoGt)\n",
    "        image_names = [cocoGt.imgs[img_id][\"file_name\"] for img_id in img_ids]\n",
    "        # inference_time = ...\n",
    "        n_gt = outcomes_per_image[:,0] + outcomes_per_image[:,2]\n",
    "        n_dt = outcomes_per_image[:,0] + outcomes_per_image[:,1]\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            precision_per_image = outcomes_per_image[:,0] / n_dt\n",
    "            recall_per_image = outcomes_per_image[:,0] / n_gt\n",
    "            f1_per_image = 2 * precision_per_image * recall_per_image / (precision_per_image + recall_per_image)\n",
    "        prediction_table = pd.DataFrame({\n",
    "            \"image_name\": image_names,\n",
    "            \"N gt\": n_gt,\n",
    "            \"N dt\": n_dt,\n",
    "            \"TP\": outcomes_per_image[:,0],\n",
    "            \"FP\": outcomes_per_image[:,1],\n",
    "            \"FN\": outcomes_per_image[:,2],\n",
    "            \"Precision\": precision_per_image,\n",
    "            \"Recall\": recall_per_image,\n",
    "            \"F1\": f1_per_image\n",
    "            })\n",
    "        return prediction_table\n",
    "    \n",
    "    \n",
    "class CalibrationMetrics:\n",
    "    def __init__(self, tp_matches, fp_matches, fn_matches, iouThrs):\n",
    "        scores = []\n",
    "        classes = []\n",
    "        iou_idxs = []\n",
    "        p_matches = tp_matches + fp_matches\n",
    "        per_class_count = defaultdict(int)\n",
    "        for m in p_matches:\n",
    "            if m['type'] == \"TP\" and m['iou'] is not None:\n",
    "                iou_idx = np.searchsorted(iouThrs, m['iou'])\n",
    "                iou_idxs.append(iou_idx)\n",
    "                assert iou_idx > 0\n",
    "            else:\n",
    "                iou_idxs.append(0)\n",
    "            scores.append(m['score'])\n",
    "            classes.append(m[\"category_id\"])\n",
    "            if m['type'] == \"TP\":\n",
    "                per_class_count[m[\"category_id\"]] += 1\n",
    "        for m in fn_matches:\n",
    "            per_class_count[m[\"category_id\"]] += 1\n",
    "        per_class_count = dict(per_class_count)\n",
    "        scores = np.array(scores)\n",
    "        inds_sort = np.argsort(-scores)\n",
    "        scores = scores[inds_sort]\n",
    "        classes = np.array(classes)[inds_sort]\n",
    "        iou_idxs = np.array(iou_idxs)[inds_sort]\n",
    "\n",
    "        self.scores = scores\n",
    "        self.classes = classes\n",
    "        self.iou_idxs = iou_idxs\n",
    "        self.per_class_count = per_class_count\n",
    "\n",
    "        # TODO What does it mean: self.iou_idxs > iou_idx\n",
    "        self.y_true = self.iou_idxs > iou_idx\n",
    "\n",
    "    def scores_vs_metrics(self, iou_idx=0, cat_id=None):\n",
    "        tps = self.iou_idxs > iou_idx\n",
    "        if cat_id is not None:\n",
    "            cls_mask = self.classes == cat_id\n",
    "            tps = tps[cls_mask]\n",
    "            scores = self.scores[cls_mask]\n",
    "            n_positives = self.per_class_count[cat_id]\n",
    "        else:\n",
    "            scores = self.scores\n",
    "            n_positives = sum(self.per_class_count.values())\n",
    "        fps = ~tps\n",
    "\n",
    "        tps_sum = tps.cumsum()\n",
    "        fps_sum = fps.cumsum()\n",
    "\n",
    "        # Precision, recall, f1\n",
    "        precision = tps_sum / (tps_sum + fps_sum)\n",
    "        recall = tps_sum / n_positives\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    def calibration_curve(self):\n",
    "        true_probs, pred_probs = calibration_curve(self.y_true, self.scores, n_bins=10)\n",
    "        return true_probs, pred_probs\n",
    "    \n",
    "    def maximum_calibration_error(self):\n",
    "        return metrics.maximum_calibration_error(self.y_true, self.scores, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Metrics(eval_data, cocoGt, cocoDt)\n",
    "m.base_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
