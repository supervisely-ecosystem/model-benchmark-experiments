{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "from model_benchmark import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data\n",
    "cocoGt=COCO(\"cocoGt.json\")\n",
    "\n",
    "# Load prediction data\n",
    "cocoDt=cocoGt.loadRes(\"cocoDt.json\")\n",
    "\n",
    "# Initialize COCOeval object\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval.params.useCats = 1\n",
    "\n",
    "# Evaluate on a subset of images (optional)\n",
    "# cocoEval.params.imgIds = [5]  # Remove this line to evaluate on all images\n",
    "\n",
    "# Run evaluation\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize COCOeval object\n",
    "cocoEval_cls = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval_cls.params.useCats = 0\n",
    "\n",
    "# Run evaluation\n",
    "cocoEval_cls.evaluate()\n",
    "cocoEval_cls.accumulate()\n",
    "cocoEval_cls.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ids = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ids = cocoEval.params.catIds\n",
    "cat_names = [cocoGt.cats[cat_id]['name'] for cat_id in cat_ids]\n",
    "\n",
    "true_positives, false_positives, false_negatives = utils.get_counts(cocoEval)\n",
    "\n",
    "eval_img_dict = utils.get_eval_img_dict(cocoEval)\n",
    "eval_img_dict_cls = utils.get_eval_img_dict(cocoEval_cls)\n",
    "matches = utils.get_matches(eval_img_dict, eval_img_dict_cls, cocoEval_cls, iou_t=0)\n",
    "\n",
    "len(matches), len(cocoDt.anns), len(cocoGt.anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {\n",
    "    \"true_positives\": true_positives,\n",
    "    \"false_positives\": false_positives,\n",
    "    \"false_negatives\": false_negatives,\n",
    "    \"matches\": matches,\n",
    "    \"coco_stats\": cocoEval.stats,\n",
    "    \"coco_precision\": cocoEval.eval['precision'],\n",
    "    \"coco_params\": cocoEval.params,\n",
    "}\n",
    "import pickle\n",
    "with open(\"eval_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(t, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_count = int(true_positives[...,0].sum())\n",
    "FP_count = int(false_positives[...,0].sum())\n",
    "FN_count = int(false_negatives[...,0].sum())\n",
    "TP_count, FP_count, FN_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_matches = [m for m in matches if m['type'] == \"TP\"]\n",
    "fp_matches = [m for m in matches if m['type'] == \"FP\"]\n",
    "fn_matches = [m for m in matches if m['type'] == \"FN\"]\n",
    "confused_matches = [m for m in fp_matches if m['miss_cls']]\n",
    "fp_not_confused_matches = [m for m in fp_matches if not m['miss_cls']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = cocoEval.stats[0]\n",
    "\n",
    "tp = true_positives.sum(1)\n",
    "fp = false_positives.sum(1)\n",
    "fn = false_negatives.sum(1)\n",
    "precision = np.mean(tp / (tp + fp))\n",
    "recall = np.mean(tp / (tp + fn))\n",
    "\n",
    "# IoU distribution\n",
    "ious = np.array([m['iou'] for m in matches if m['iou']])\n",
    "iou_hist = np.histogram(ious, range=(0.5, 1))\n",
    "iou_mean = np.mean(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "tp = true_positives.sum(1).mean(1)\n",
    "fp = false_positives.sum(1).mean(1)\n",
    "fn = false_negatives.sum(1).mean(1)\n",
    "\n",
    "pr = tp / (tp + fp)\n",
    "rc = tp / (tp + fn)\n",
    "\n",
    "pr_sort = np.argsort(pr)\n",
    "rc_sort = np.argsort(rc)\n",
    "pr_names = [cat_names[i] for i in pr_sort]\n",
    "rc_names = [cat_names[i] for i in rc_sort]\n",
    "pr_values = pr[pr_sort]\n",
    "rc_values = rc[rc_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP for precision\n",
    "# FN for recall\n",
    "\n",
    "if save_ids:\n",
    "    pr_per_class_ids = {cat_name: [] for cat_name in cat_names}\n",
    "    for m in fp_matches:\n",
    "        cat_id = m['category_id']\n",
    "        cat_name = cocoGt.cats[cat_id]['name']\n",
    "        pr_per_class_ids[cat_name].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "    rc_per_class_ids = {cat_name: [] for cat_name in cat_names}\n",
    "    for m in fn_matches:\n",
    "        cat_id = m['category_id']\n",
    "        cat_name = cocoGt.cats[cat_id]['name']\n",
    "        rc_per_class_ids[cat_name].append((m['image_id'], m['gt_id'], m['dt_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: R x K\n",
    "pr_curve_np = cocoEval.eval['precision'][:,:,:,0,2].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confuse_count = len(confused_matches)\n",
    "classification_accuracy = TP_count / (TP_count + confuse_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "catId2idx = {cat_id: i for i, cat_id in enumerate(cat_ids)}\n",
    "idx2catId = {i: cat_id for cat_id, i in catId2idx.items()}\n",
    "\n",
    "confusion_matrix = np.zeros((len(cat_ids)+1, len(cat_ids)+1), dtype=int)\n",
    "confusion_matrix_ids = [[[] for _ in range(len(cat_ids)+1)] for _ in range(len(cat_ids)+1)]\n",
    "\n",
    "for m in confused_matches:\n",
    "    cat_idx_pred = catId2idx[m['category_id']]\n",
    "    cat_idx_gt = catId2idx[cocoGt.anns[m['gt_id']]['category_id']]\n",
    "    confusion_matrix[cat_idx_pred, cat_idx_gt] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[cat_idx_pred][cat_idx_gt].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "for m in tp_matches:\n",
    "    cat_idx = catId2idx[m['category_id']]\n",
    "    confusion_matrix[cat_idx, cat_idx] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[cat_idx][cat_idx].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "for m in fp_not_confused_matches:\n",
    "    cat_idx_pred = catId2idx[m['category_id']]\n",
    "    confusion_matrix[cat_idx_pred, -1] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[cat_idx_pred][-1].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "for m in fn_matches:\n",
    "    cat_idx_gt = catId2idx[m['category_id']]\n",
    "    confusion_matrix[-1, cat_idx_gt] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[-1][cat_idx_gt].append((m['image_id'], m['gt_id'], m['dt_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequently confused class pairs\n",
    "topk_pairs = 20\n",
    "cm = confusion_matrix[:-1,:-1]\n",
    "cm_l = np.tril(cm, -1)\n",
    "cm_u = np.triu(cm, 1)\n",
    "cm = cm_l + cm_u.T\n",
    "cm_flat = cm.flatten()\n",
    "inds_sort = np.argsort(-cm_flat)[:topk_pairs]\n",
    "inds_sort = inds_sort[cm_flat[inds_sort] > 0]  # remove zeros\n",
    "inds_sort = np.unravel_index(inds_sort, cm.shape)\n",
    "\n",
    "# probability of confusion: (predicted A, actually B + predicted B, actually A) / (predicted A + predicted B)\n",
    "confused_counts = cm[inds_sort]\n",
    "dt_total = confusion_matrix.sum(1)\n",
    "dt_pair_sum = np.array([dt_total[i] + dt_total[j] for i, j in zip(*inds_sort)])\n",
    "confused_prob = confused_counts / dt_pair_sum\n",
    "inds_sort2 = np.argsort(-confused_prob)\n",
    "\n",
    "confused_idxs = np.array(inds_sort).T[inds_sort2]\n",
    "confused_name_pairs = [(cat_names[i], cat_names[j]) for i, j in confused_idxs]\n",
    "confused_counts = confused_counts[inds_sort2]\n",
    "confused_prob = confused_prob[inds_sort2]\n",
    "confused_catIds = [(idx2catId[i], idx2catId[j]) for i, j in confused_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "if save_ids:\n",
    "    frequent_confusion_ids = {}\n",
    "    for i, j in confused_idxs:\n",
    "        c_ids = confusion_matrix_ids[i][j] + confusion_matrix_ids[j][i]\n",
    "        random.shuffle(c_ids)\n",
    "        cat_i, cat_j = idx2catId[i], idx2catId[j]\n",
    "        frequent_confusion_ids[(cat_i, cat_j)] = c_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class AP\n",
    "pr = cocoEval.eval['precision'][:, :, :, 0, 2]\n",
    "ap_per_class = pr.mean(axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Counts\n",
    "iou_thres = 0\n",
    "\n",
    "tp = true_positives.sum(1)[:,iou_thres]\n",
    "fp = false_positives.sum(1)[:,iou_thres]\n",
    "fn = false_negatives.sum(1)[:,iou_thres]\n",
    "\n",
    "# normalize\n",
    "support = tp + fn\n",
    "tp_rel = tp / support\n",
    "fp_rel = fp / support\n",
    "fn_rel = fn / support\n",
    "\n",
    "# sort by tp_rel / fp_rel / fn_rel\n",
    "# sort_scores = tp_rel - fp_rel - fn_rel\n",
    "# sort_scores = ap_per_class\n",
    "# f1\n",
    "sort_scores = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "K = len(cat_names)\n",
    "sort_indices = np.argsort(sort_scores)\n",
    "cat_names_sorted = [cat_names[i] for i in sort_indices]\n",
    "tp_rel, fn_rel, fp_rel = tp_rel[sort_indices], fn_rel[sort_indices], fp_rel[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_ids:\n",
    "    per_class_counts_ids = {cat_name: {\"TP\": [], \"FN\": [], \"FP\": []} for cat_name in cat_names_sorted}\n",
    "\n",
    "    for m in tp_matches:\n",
    "        cat_id = m['category_id']\n",
    "        cat_name = cocoGt.cats[cat_id]['name']\n",
    "        per_class_counts_ids[cat_name][\"TP\"].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "    for m in fn_matches:\n",
    "        cat_id = m['category_id']\n",
    "        cat_name = cocoGt.cats[cat_id]['name']\n",
    "        per_class_counts_ids[cat_name][\"FN\"].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "    for m in fp_matches:\n",
    "        cat_id = m['category_id']\n",
    "        cat_name = cocoGt.cats[cat_id]['name']\n",
    "        per_class_counts_ids[cat_name][\"FP\"].append((m['image_id'], m['gt_id'], m['dt_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_vs_metrics(tp_matches, fp_matches, fn_matches, iouThrs):\n",
    "    scores = []\n",
    "    classes = []\n",
    "    iou_idxs = []\n",
    "    p_matches = tp_matches + fp_matches\n",
    "    per_class_count = defaultdict(int)\n",
    "    for m in p_matches:\n",
    "        if m['type'] == \"TP\" and m['iou'] is not None:\n",
    "            iou_idx = np.searchsorted(iouThrs, m['iou'])\n",
    "            iou_idxs.append(iou_idx)\n",
    "            assert iou_idx > 0\n",
    "        else:\n",
    "            iou_idxs.append(0)\n",
    "        scores.append(m['score'])\n",
    "        classes.append(m[\"category_id\"])\n",
    "        if m['type'] == \"TP\":\n",
    "            per_class_count[m[\"category_id\"]] += 1\n",
    "    for m in fn_matches:\n",
    "        per_class_count[m[\"category_id\"]] += 1\n",
    "    per_class_count = dict(per_class_count)\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    inds_sort = np.argsort(-scores)\n",
    "    scores = scores[inds_sort]\n",
    "    classes = np.array(classes)[inds_sort]\n",
    "    iou_idxs = np.array(iou_idxs)[inds_sort]\n",
    "\n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"classes\": classes,\n",
    "        \"iou_idxs\": iou_idxs,\n",
    "        \"per_class_count\": per_class_count\n",
    "    }\n",
    "\n",
    "\n",
    "class ScoresVsMetrics:\n",
    "    def __init__(self, scores, classes, iou_idxs, per_class_count):\n",
    "        self.scores = scores\n",
    "        self.classes = classes\n",
    "        self.iou_idxs = iou_idxs\n",
    "        self.per_class_count = per_class_count\n",
    "\n",
    "    def query(self, iou_idx=0, cat_id=None):\n",
    "        tps = self.iou_idxs > iou_idx\n",
    "        if cat_id is not None:\n",
    "            cls_mask = self.classes == cat_id\n",
    "            tps = tps[cls_mask]\n",
    "            scores = self.scores[cls_mask]\n",
    "            n_positives = self.per_class_count[cat_id]\n",
    "        else:\n",
    "            scores = self.scores\n",
    "            n_positives = sum(self.per_class_count.values())\n",
    "        fps = ~tps\n",
    "\n",
    "        tps_sum = tps.cumsum()\n",
    "        fps_sum = fps.cumsum()\n",
    "\n",
    "        # Precision, recall, f1\n",
    "        precision = tps_sum / (tps_sum + fps_sum)\n",
    "        recall = tps_sum / n_positives\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return scores, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_vs_metrics_result = calculate_scores_vs_metrics(tp_matches, fp_matches, fn_matches, cocoEval.params.iouThrs)\n",
    "\n",
    "iou_idx = 0\n",
    "cat_id = None\n",
    "scores, classes, iou_idxs, per_class_count = scores_vs_metrics_result.values()\n",
    "# s = ScoresVsMetrics(scores, classes, iou_idxs, per_class_count)\n",
    "# scores, precision, recall, f1 = s.query(iou_idx, cat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist + KDE\n",
    "tps = iou_idxs > iou_idx\n",
    "scores_tp = scores[tps]\n",
    "scores_fp = scores[~tps]\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "x = np.linspace(0., 1, 500, endpoint=False)\n",
    "kde_tp = gaussian_kde(scores_tp)\n",
    "kde_fp = gaussian_kde(scores_fp)\n",
    "density_tp = kde_tp(x)\n",
    "density_fp = kde_fp(x)\n",
    "\n",
    "# density is relative to the number of data points\n",
    "# density_tp *= len(scores_tp)\n",
    "# density_fp *= len(scores_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = iou_idxs > iou_idx\n",
    "y_pred = iou_idxs >= iou_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import model_benchmark.metrics as metrics\n",
    "true_probs, pred_probs = calibration_curve(y_true, scores, n_bins=10)\n",
    "brier_score = 1 - brier_score_loss(y_true, scores)\n",
    "\n",
    "mce = metrics.maximum_calibration_error(y_true, scores, n_bins=10)\n",
    "print(f\"Expected Calibration Error (ECE): {mce:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcomes_per_image(matches, cocoGt: COCO):\n",
    "    img_ids = sorted(cocoGt.getImgIds())\n",
    "    imgId2idx = {img_id: idx for idx, img_id in enumerate(img_ids)}\n",
    "    outcomes_per_image = np.zeros((len(img_ids), 3), dtype=float)\n",
    "    for m in matches:\n",
    "        img_id = m[\"image_id\"]\n",
    "        idx = imgId2idx[img_id]\n",
    "        if m[\"type\"] == \"TP\":\n",
    "            outcomes_per_image[idx, 0] += 1\n",
    "        elif m[\"type\"] == \"FP\":\n",
    "            outcomes_per_image[idx, 1] += 1\n",
    "        elif m[\"type\"] == \"FN\":\n",
    "            outcomes_per_image[idx, 2] += 1\n",
    "    return img_ids, outcomes_per_image\n",
    "\n",
    "img_ids, outcomes_per_image = get_outcomes_per_image(matches, cocoGt)\n",
    "image_names = [cocoGt.imgs[img_id][\"file_name\"] for img_id in img_ids]\n",
    "# inference_time = ...\n",
    "n_gt = outcomes_per_image[:,0] + outcomes_per_image[:,2]\n",
    "n_dt = outcomes_per_image[:,0] + outcomes_per_image[:,1]\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    precision_per_image = outcomes_per_image[:,0] / n_dt\n",
    "    recall_per_image = outcomes_per_image[:,0] / n_gt\n",
    "    f1_per_image = 2 * precision_per_image * recall_per_image / (precision_per_image + recall_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_table = pd.DataFrame({\n",
    "    \"image_name\": image_names,\n",
    "    \"N gt\": n_gt,\n",
    "    \"N dt\": n_dt,\n",
    "    \"TP\": outcomes_per_image[:,0],\n",
    "    \"FP\": outcomes_per_image[:,1],\n",
    "    \"FN\": outcomes_per_image[:,2],\n",
    "    \"Precision\": precision_per_image,\n",
    "    \"Recall\": recall_per_image,\n",
    "    \"F1\": f1_per_image\n",
    "    })\n",
    "prediction_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "tp = true_positives.sum(0)[:,t]\n",
    "fp = false_positives.sum(0)[:,t]\n",
    "fn = false_negatives.sum(0)[:,t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "y_edges = np.arange(min(tp) - 0.5, max(tp) + 1.5, 1)\n",
    "x_edges = np.arange(min(fp+fn) - 0.5, max(fp+fn) + 1.5, 1)\n",
    "heatmap, y_edges, x_edges = np.histogram2d(tp, fp+fn, bins=(y_edges, x_edges))\n",
    "\n",
    "z_max = np.max(heatmap)\n",
    "gamma = 0.95\n",
    "\n",
    "colors = np.zeros((heatmap.shape[0], heatmap.shape[1], 3))  # for RGB channels\n",
    "\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        tp_val = x_edges[j] + 0.5\n",
    "        fp_fn_val = y_edges[i] + 0.5\n",
    "        \n",
    "        intensity = heatmap[i, j]\n",
    "        if tp_val + fp_fn_val > 0:\n",
    "            value = tp_val / (tp_val + fp_fn_val)\n",
    "        else:\n",
    "            value = 0\n",
    "        \n",
    "        # green to red colormap\n",
    "        colormap_name = 'RdYlGn_r'\n",
    "        color = cm.get_cmap(colormap_name)(value)  # Get a color from a colormap\n",
    "        # Adjust the color intensity based on the heatmap value\n",
    "        if intensity > 0:\n",
    "            c = np.array(color[:3]) * max(0.2, np.log(intensity) / np.log(z_max))\n",
    "            colors[i, j, :] = c**gamma\n",
    "        else:\n",
    "            colors[i, j, :] = np.array(color[:3]) * 0.12\n",
    "\n",
    "# Plot the colored heatmap\n",
    "fig = px.imshow(colors, labels=dict(x=\"Count of Errors\", y=\"Count of True Predictions\"), title=\"TP vs FP+FN\", text_auto=True, origin='lower',\n",
    "                width=800, height=800)\n",
    "\n",
    "# Adding text to each pixel\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        fig.add_annotation(\n",
    "            x=j, \n",
    "            y=i, \n",
    "            text=str(int(heatmap[i, j])),\n",
    "            showarrow=False,\n",
    "            font=dict(color=\"#ddd\", size=10)\n",
    "        )\n",
    "\n",
    "# Remove margin\n",
    "# fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_metrics = {\n",
    "    \"mAP\": mAP,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"iou\": iou_mean,\n",
    "    \"classification_accuracy\": classification_accuracy,\n",
    "    \"reliability\": brier_score,\n",
    "}\n",
    "\n",
    "recall_metrics = {\n",
    "    \"recall\": recall,\n",
    "    \"per_class\": {\"class_names\": rc_names, \"recall\": rc_values},\n",
    "    \"TP\": TP_count,\n",
    "    \"TP+FN\": TP_count + FN_count,\n",
    "}\n",
    "\n",
    "precision_metrics = {\n",
    "    \"precision\": precision,\n",
    "    \"per_class\": {\"class_names\": pr_names, \"precision\": pr_values},\n",
    "    \"TP\": TP_count,\n",
    "    \"TP+FP\": TP_count + FP_count,\n",
    "}\n",
    "\n",
    "pr_curve = pr_curve_np\n",
    "\n",
    "classification_metrics = {\n",
    "    \"classification_accuracy\": classification_accuracy,\n",
    "    \"confuse_count\": confuse_count,\n",
    "    \"total\": TP_count + confuse_count,\n",
    "    \"confusion_matrix\": confusion_matrix,\n",
    "    \"frequently_confused\": {\"class_name_pairs\": confused_name_pairs, \"counts\": confused_counts, \"prob\": confused_prob},\n",
    "}\n",
    "\n",
    "localization_metrics = {\n",
    "    \"iou\": iou_mean,\n",
    "    \"iou_hist\": iou_hist,\n",
    "}\n",
    "\n",
    "per_class_metrics = {\n",
    "    \"AP\": ap_per_class,\n",
    "    \"per_class_counts\": {\"class_names\": cat_names_sorted, \"TP\": tp_rel, \"FN\": fn_rel, \"FP\": fp_rel},\n",
    "}\n",
    "\n",
    "confidence_metrics = {\n",
    "    \"confidence_vs_metrics\": scores_vs_metrics_result,\n",
    "    \"confidence_histogram\": \"note: can be made with confidence_vs_metrics\",\n",
    "    \"brier_score\": brier_score,\n",
    "    \"calibration_curve\": {\"true_probs\": true_probs, \"pred_probs\": pred_probs},\n",
    "}\n",
    "\n",
    "if save_ids:\n",
    "    json_ids = {\n",
    "        \"recall_metrics\": {\n",
    "            \"per_class\": rc_per_class_ids,\n",
    "        },\n",
    "        \"precision_metrics\": {\n",
    "            \"per_class\": pr_per_class_ids,\n",
    "        },\n",
    "        \"classification_metrics\": {\n",
    "            \"confusion_matrix\": confusion_matrix_ids,\n",
    "            \"frequently_confused\": frequent_confusion_ids,\n",
    "        },\n",
    "        \"per_class_metrics\": {\n",
    "            \"per_class_counts\": per_class_counts_ids,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P/R vs IoU\")\n",
    "tp = true_positives.sum((0,1))\n",
    "fp = false_positives.sum((0,1))\n",
    "fn = false_negatives.sum((0,1))\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "plt.plot(recall, label=\"Recall\")\n",
    "plt.plot(precision, label=\"Precision\")\n",
    "plt.legend()\n",
    "plt.title(\"Precision / Recall vs IoU\")\n",
    "plt.xlabel(\"IoU\")\n",
    "plt.ylabel(\"Precision / Recall\")\n",
    "plt.xticks(range(0, len(cocoEval.params.iouThrs), 1), cocoEval.params.iouThrs[::1])\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_benchmark.prediction_gallery import prediction_gallery\n",
    "cat_ids_rare, cat_names_rare = utils.get_rare_classes(cocoGt)\n",
    "\n",
    "gallery = prediction_gallery(matches, cocoGt, cat_ids_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show confusion matrix with plotly\n",
    "confusion_matrix_df = pd.DataFrame(np.log(confusion_matrix), index=cat_names + ['(background)'], columns=cat_names + ['(background)'])\n",
    "fig = px.imshow(confusion_matrix_df, labels=dict(x=\"Predicted\", y=\"Ground Truth\", color=\"Count\"), title=\"Confusion Matrix (log scale)\",\n",
    "                width=800, height=800)\n",
    "# remove margin\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=50, b=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw frequency of confusion as bar chart with plotly\n",
    "x_labels = [f\"{pair[0]} - {pair[1]}\" for pair in confused_name_pairs]\n",
    "fig = go.Figure()\n",
    "color_scale = px.colors.sequential.Reds\n",
    "fig.add_trace(go.Bar(x=x_labels, y=confused_prob, marker=dict(color=confused_prob, colorscale=color_scale)))\n",
    "fig.update_layout(title=\"Frequently confused class pairs\", xaxis_title=\"Class pair\", yaxis_title=\"Probability\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Average Precision (AP)\n",
    "fig = px.scatter_polar(r=ap_per_class, theta=cat_names, title=\"Per-class Average Precision (AP)\",\n",
    "                       labels=dict(r=\"Average Precision\", theta=\"Category\"),\n",
    "                       width=600, height=600,\n",
    "                       range_r=[0, 1])\n",
    "# fill points\n",
    "fig.update_traces(fill='toself')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked per-class counts\n",
    "data = {\n",
    "    \"count\": np.concatenate([tp_rel, fn_rel, fp_rel]),\n",
    "    \"type\": [\"TP\"]*K + [\"FN\"]*K + [\"FP\"]*K,\n",
    "    \"category\": cat_names_sorted*3\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "color_map = {\n",
    "    'TP': '#1fb466',\n",
    "    'FN': '#dd3f3f',\n",
    "    'FP': '#d5a5a5'\n",
    "}\n",
    "fig = px.bar(df, x=\"category\", y=\"count\", color=\"type\", title=\"Per-class Outcome Counts\",\n",
    "             labels={'count': 'Total Count'},# text='count',\n",
    "             color_discrete_map=color_map)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked per-class counts (absolute)\n",
    "data = {\n",
    "    \"count\": np.concatenate([tp[sort_indices], fn[sort_indices], fp[sort_indices]]),\n",
    "    \"type\": [\"TP\"]*K + [\"FN\"]*K + [\"FP\"]*K,\n",
    "    \"category\": cat_names_sorted*3\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "color_map = {\n",
    "    'TP': '#1fb466',\n",
    "    'FN': '#dd3f3f',\n",
    "    'FP': '#d5a5a5'\n",
    "}\n",
    "fig = px.bar(df, x=\"category\", y=\"count\", color=\"type\", title=\"Per-class Outcome Counts\",\n",
    "             labels={'count': 'Total Count'},# text='count',\n",
    "             color_discrete_map=color_map)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence vs F1\n",
    "s = ScoresVsMetrics(scores, classes, iou_idxs, per_class_count)\n",
    "scores2, precision2, recall2, f1_2 = s.query(iou_idx, cat_id)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scores2, y=precision2, mode='lines', name='Precision'))\n",
    "fig.add_trace(go.Scatter(x=scores2, y=recall2, mode='lines', name='Recall'))\n",
    "fig.add_trace(go.Scatter(x=scores2, y=f1_2, mode='lines', name='F1'))\n",
    "\n",
    "fig.update_layout(title=\"Precision, Recall, F1 vs Confidence Score\",\n",
    "                    xaxis_title=\"Confidence Score\", yaxis_title=\"Value\",\n",
    "                    width=800, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, density_tp, label=\"TP\")\n",
    "plt.plot(x, density_fp, label=\"FP\")\n",
    "y_hist, x_hist = np.histogram(scores_tp, bins=50, density=True)\n",
    "dx = x_hist[1] - x_hist[0]\n",
    "plt.bar(x_hist[:-1]+dx/2, y_hist, width=dx, alpha=0.5)\n",
    "y_hist, x_hist = np.histogram(scores_fp, bins=50, density=True)\n",
    "dx = x_hist[1] - x_hist[0]\n",
    "plt.bar(x_hist[:-1]+dx/2, y_hist, width=dx, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Confidence Score Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(scores_tp, bins=50, alpha=0.5, label=\"TP\")\n",
    "plt.hist(scores_fp, bins=50, alpha=0.5, label=\"FP\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð¡alibration curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(pred_probs, true_probs, marker='o', linewidth=1, label='Calibration plot (Model)')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Fraction of True Positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome counts\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.barh([\"TP\", \"FN\", \"FP\"], [TP_count, FN_count, FP_count], color=['#1fb466', '#dd3f3f', '#d5a5a5'])\n",
    "plt.gca().invert_yaxis()\n",
    "# plt.barh(0, TP_count, color='#1fb466', label='TP')\n",
    "# plt.barh(0, FN_count, left=TP_count, color='#dd3f3f', label='FN')\n",
    "# plt.barh(0, FP_count, left=TP_count+FN_count, color='#d5a5a5', label='FP')\n",
    "plt.legend()\n",
    "plt.title(\"Outcome Counts\")\n",
    "# plt.yticks([])\n",
    "plt.ylabel(\"Outcome\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tmp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU distribution, plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=iou_hist[1], y=iou_hist[0]))\n",
    "fig.update_layout(title=\"IoU Distribution\", xaxis_title=\"IoU\", yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
