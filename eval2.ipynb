{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import utils\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=6.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.37s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.602\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.503\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.263\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.517\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.659\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.354\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.525\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.583\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.734\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth data\n",
    "cocoGt=COCO(\"cocoGt.json\")\n",
    "\n",
    "# Load prediction data\n",
    "cocoDt=cocoGt.loadRes(\"cocoDt.json\")\n",
    "\n",
    "# Initialize COCOeval object\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval.params.useCats = 1\n",
    "\n",
    "# Evaluate on a subset of images (optional)\n",
    "# cocoEval.params.imgIds = [5]  # Remove this line to evaluate on all images\n",
    "\n",
    "# Run evaluation\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=7.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.47s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.635\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.518\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.269\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.561\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.732\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.532\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.307\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.624\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.799\n"
     ]
    }
   ],
   "source": [
    "# Initialize COCOeval object\n",
    "cocoEval_cls = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval_cls.params.useCats = 0\n",
    "\n",
    "# Run evaluation\n",
    "cocoEval_cls.evaluate()\n",
    "cocoEval_cls.accumulate()\n",
    "cocoEval_cls.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increases score:\n",
    "# prediction with high confidence but incorrect (FP)\n",
    "# TP with low confidence, low IoU\n",
    "# edge cases: small objects, big objects, low IoU, low confidence\n",
    "# many predictions in one image\n",
    "# many FP predictions in one image\n",
    "# many FN in one image\n",
    "# miss-classified objects with high/low confidence\n",
    "# miss-classified prediciotn, and it is the most often miss-classified class\n",
    "\n",
    "# finding insightful FN:\n",
    "# many FN in one image\n",
    "# FN with small area\n",
    "# FN on the edge\n",
    "# FN with tall/len object\n",
    "\n",
    "# finding insightful TP:\n",
    "# TP with low confidence\n",
    "# TP with low IoU\n",
    "# TP with small area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-image stats:\n",
    "# prediction count (TP+FP)\n",
    "# FP count\n",
    "# FN count\n",
    "# TP + low conf + low IoU (avg on image)\n",
    "# FP + high conf (avg on image)\n",
    "# many rare classes (avg)\n",
    "\n",
    "# per-prediction stats:\n",
    "# TP + low conf\n",
    "# TP + low IoU\n",
    "# FP + high conf\n",
    "# rare class\n",
    "# TP, but incorrect class + high conf\n",
    "# FN + small area (?)\n",
    "\n",
    "# image_id / ann_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ids = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44425, 33765, 36781)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_ids = cocoEval.params.catIds\n",
    "cat_names = [cocoGt.cats[cat_id]['name'] for cat_id in cat_ids]\n",
    "\n",
    "true_positives, false_positives, false_negatives = utils.get_counts(cocoEval)\n",
    "\n",
    "eval_img_dict = utils.get_eval_img_dict(cocoEval)\n",
    "eval_img_dict_cls = utils.get_eval_img_dict(cocoEval_cls)\n",
    "matches = utils.get_matches(eval_img_dict, eval_img_dict_cls, cocoEval_cls, iou_t=0)\n",
    "\n",
    "len(matches), len(cocoDt.anns), len(cocoGt.anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24102, 8090, 12233)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP_count = int(true_positives[...,0].sum())\n",
    "FP_count = int(false_positives[...,0].sum())\n",
    "FN_count = int(false_negatives[...,0].sum())\n",
    "TP_count, FP_count, FN_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = cocoEval.stats[0]\n",
    "\n",
    "tp = true_positives.sum(1)\n",
    "fp = false_positives.sum(1)\n",
    "fn = false_negatives.sum(1)\n",
    "precision = np.mean(tp / (tp + fp))\n",
    "recall = np.mean(tp / (tp + fn))\n",
    "\n",
    "# IoU distribution\n",
    "ious = np.array([m['iou'] for m in matches if m['iou']])\n",
    "iou_hist = np.histogram(ious, range=(0.5, 1))\n",
    "iou_mean = np.mean(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "tp = true_positives.sum(1).mean(1)\n",
    "fp = false_positives.sum(1).mean(1)\n",
    "fn = false_negatives.sum(1).mean(1)\n",
    "\n",
    "pr = tp / (tp + fp)\n",
    "rc = tp / (tp + fn)\n",
    "\n",
    "pr_sort = np.argsort(pr)\n",
    "rc_sort = np.argsort(rc)\n",
    "pr_names = [cat_names[i] for i in pr_sort]\n",
    "rc_names = [cat_names[i] for i in rc_sort]\n",
    "pr_values = pr[pr_sort]\n",
    "rc_values = rc[rc_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_matches = [m for m in matches if m['type'] == \"TP\"]\n",
    "fp_matches = [m for m in matches if m['type'] == \"FP\" and not m['miss_cls']]\n",
    "fn_matches = [m for m in matches if m['type'] == \"FN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP for precision\n",
    "# FN for recall\n",
    "\n",
    "if save_ids:\n",
    "    pr_per_class_ids = defaultdict(list)\n",
    "    for m in fp_matches:\n",
    "        cat_id = m['category_id']\n",
    "        cat_name = cocoGt.cats[cat_id]['name']\n",
    "        pr_per_class_ids[cat_name].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "    pr_per_class_ids = dict(pr_per_class_ids)\n",
    "\n",
    "    rc_per_class_ids = defaultdict(list)\n",
    "    for m in fn_matches:\n",
    "        cat_id = m['category_id']\n",
    "        cat_name = cocoGt.cats[cat_id]['name']\n",
    "        rc_per_class_ids[cat_name].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "    rc_per_class_ids = dict(rc_per_class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_curve shape: R x K\n",
    "pr_curve = cocoEval.eval['precision'][:,:,:,0,2].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_cls = [m for m in matches if m['miss_cls']]\n",
    "confuse_count = len(miss_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "catId2idx = {cat_id: i for i, cat_id in enumerate(cat_ids)}\n",
    "\n",
    "confusion_matrix = np.zeros((len(cat_ids)+1, len(cat_ids)+1))\n",
    "confusion_matrix_ids = [[[] for _ in range(len(cat_ids)+1)] for _ in range(len(cat_ids)+1)]\n",
    "\n",
    "for m in miss_cls:\n",
    "    cat_idx_pred = catId2idx[m['category_id']]\n",
    "    cat_idx_gt = catId2idx[cocoGt.anns[m['gt_id']]['category_id']]\n",
    "    confusion_matrix[cat_idx_pred, cat_idx_gt] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[cat_idx_pred][cat_idx_gt].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "for m in tp_matches:\n",
    "    cat_idx = catId2idx[m['category_id']]\n",
    "    confusion_matrix[cat_idx, cat_idx] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[cat_idx][cat_idx].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "for m in fp_matches:\n",
    "    cat_idx_pred = catId2idx[m['category_id']]\n",
    "    confusion_matrix[cat_idx_pred, -1] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[cat_idx_pred][-1].append((m['image_id'], m['gt_id'], m['dt_id']))\n",
    "\n",
    "for m in fn_matches:\n",
    "    cat_idx_gt = catId2idx[m['category_id']]\n",
    "    confusion_matrix[-1, cat_idx_gt] += 1\n",
    "    if save_ids:\n",
    "        confusion_matrix_ids[-1][cat_idx_gt].append((m['image_id'], m['gt_id'], m['dt_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show confusion matrix with plotly\n",
    "confusion_matrix_df = pd.DataFrame(np.log(confusion_matrix), index=cat_names + ['(background)'], columns=cat_names + ['(background)'])\n",
    "fig = px.imshow(confusion_matrix_df, labels=dict(x=\"Predicted\", y=\"Ground Truth\", color=\"Count\"), title=\"Confusion Matrix (log scale)\",\n",
    "                width=800, height=800)\n",
    "# remove margin\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=50, b=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_metrics = {\n",
    "    \"mAP\": mAP,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"iou\": iou_mean,\n",
    "}\n",
    "\n",
    "recall_metrics = {\n",
    "    \"recall\": recall,\n",
    "    \"per_class\": {\"class_names\": rc_names, \"recall\": rc_values},\n",
    "    \"TP\": TP_count,\n",
    "    \"TP+FN\": TP_count + FN_count,\n",
    "}\n",
    "\n",
    "precision_metrics = {\n",
    "    \"precision\": precision,\n",
    "    \"per_class\": {\"class_names\": pr_names, \"precision\": pr_values},\n",
    "    \"TP\": TP_count,\n",
    "    \"TP+FP\": TP_count + FP_count,\n",
    "}\n",
    "\n",
    "pr_curve = pr_curve\n",
    "\n",
    "classification_metrics = {\n",
    "    \"classification_accuracy\": TP_count / (TP_count + confuse_count),\n",
    "    \"confuse_count\": confuse_count,\n",
    "    \"total\": TP_count + confuse_count,\n",
    "    \"confusion_matrix\": confusion_matrix,\n",
    "}\n",
    "\n",
    "localization_metrics = {\n",
    "    \"iou\": iou_mean,\n",
    "    \"iou_hist\": iou_hist,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P/R vs IoU\")\n",
    "tp = true_positives.sum((0,1))\n",
    "fp = false_positives.sum((0,1))\n",
    "fn = false_negatives.sum((0,1))\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "plt.plot(recall)\n",
    "plt.plot(precision)\n",
    "plt.title(\"Precision / Recall vs IoU\")\n",
    "plt.xlabel(\"IoU\")\n",
    "plt.ylabel(\"Precision / Recall\")\n",
    "plt.xticks(range(0, len(cocoEval.params.iouThrs), 1), cocoEval.params.iouThrs[::1])\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/model-benchmark/prediction_gallery.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  per_image[:, 3] /= (per_image[:, 0] + per_image[:, 1])\n",
      "/root/model-benchmark/prediction_gallery.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  per_image[:, 4] /= per_image[:, 0]\n"
     ]
    }
   ],
   "source": [
    "from prediction_gallery import prediction_gallery\n",
    "cat_ids_rare, cat_names_rare = utils.get_rare_classes(cocoGt)\n",
    "\n",
    "gallery = prediction_gallery(matches, cocoGt, cat_ids_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
