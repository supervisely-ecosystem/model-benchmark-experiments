{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import utils\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=6.52s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.37s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.602\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.503\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.263\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.517\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.659\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.354\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.525\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.583\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.734\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth data\n",
    "cocoGt=COCO(\"cocoGt.json\")\n",
    "\n",
    "# Load prediction data\n",
    "cocoDt=cocoGt.loadRes(\"cocoDt.json\")\n",
    "\n",
    "# Initialize COCOeval object\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval.params.useCats = 1\n",
    "\n",
    "# Evaluate on a subset of images (optional)\n",
    "# cocoEval.params.imgIds = [5]  # Remove this line to evaluate on all images\n",
    "\n",
    "# Run evaluation\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=7.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.635\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.518\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.269\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.561\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.732\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.532\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.307\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.624\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.799\n"
     ]
    }
   ],
   "source": [
    "# Initialize COCOeval object\n",
    "cocoEval_cls = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval_cls.params.useCats = 0\n",
    "\n",
    "# Run evaluation\n",
    "cocoEval_cls.evaluate()\n",
    "cocoEval_cls.accumulate()\n",
    "cocoEval_cls.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increases score:\n",
    "# prediction with high confidence but incorrect (FP)\n",
    "# TP with low confidence, low IoU\n",
    "# edge cases: small objects, big objects, low IoU, low confidence\n",
    "# many predictions in one image\n",
    "# many FP predictions in one image\n",
    "# many FN in one image\n",
    "# miss-classified objects with high/low confidence\n",
    "# miss-classified prediciotn, and it is the most often miss-classified class\n",
    "\n",
    "# finding insightful FN:\n",
    "# many FN in one image\n",
    "# FN with small area\n",
    "# FN on the edge\n",
    "# FN with tall/len object\n",
    "\n",
    "# finding insightful TP:\n",
    "# TP with low confidence\n",
    "# TP with low IoU\n",
    "# TP with small area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-image stats:\n",
    "# prediction count (TP+FP)\n",
    "# FP count\n",
    "# FN count\n",
    "# TP + low conf + low IoU (avg on image)\n",
    "# FP + high conf (avg on image)\n",
    "# many rare classes (avg)\n",
    "\n",
    "# per-prediction stats:\n",
    "# TP + low conf\n",
    "# TP + low IoU\n",
    "# FP + high conf\n",
    "# rare class\n",
    "# TP, but incorrect class + high conf\n",
    "# FN + small area (?)\n",
    "\n",
    "# image_id / ann_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/root/model-benchmark/utils.py'>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_img_dict = utils.get_eval_img_dict(cocoEval)\n",
    "eval_img_dict_cls = utils.get_eval_img_dict(cocoEval_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = utils.get_matches(eval_img_dict, eval_img_dict_cls, cocoEval_cls, iou_t=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44425, 33765, 36781)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches), len(cocoDt.anns), len(cocoGt.anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-image stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (N_imgs, 5), 5 = TP, FP, FN, score, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_334644/1506004393.py:22: RuntimeWarning: invalid value encountered in divide\n",
      "  per_image[:, 3] /= (per_image[:, 0] + per_image[:, 1])\n",
      "/tmp/ipykernel_334644/1506004393.py:23: RuntimeWarning: invalid value encountered in divide\n",
      "  per_image[:, 4] /= per_image[:, 0]\n"
     ]
    }
   ],
   "source": [
    "# Per-image stats\n",
    "img_ids = cocoEval.params.imgIds\n",
    "\n",
    "per_image = np.zeros((len(img_ids), 5))\n",
    "\n",
    "imgId2idx = {img_id: idx for idx, img_id in enumerate(img_ids)}\n",
    "idx2imgId = {idx: img_id for img_id, idx in imgId2idx.items()}\n",
    "\n",
    "for match in matches:\n",
    "    idx = imgId2idx[match['image_id']]\n",
    "    if match[\"type\"] == \"TP\":\n",
    "        per_image[idx, 0] += 1\n",
    "        per_image[idx, 3] += match[\"score\"]\n",
    "        per_image[idx, 4] += match[\"iou\"]\n",
    "    elif match[\"type\"] == \"FP\":\n",
    "        per_image[idx, 1] += 1\n",
    "        per_image[idx, 3] += match[\"score\"]\n",
    "    elif match[\"type\"] == \"FN\":\n",
    "        per_image[idx, 2] += 1\n",
    "\n",
    "\n",
    "per_image[:, 3] /= (per_image[:, 0] + per_image[:, 1])\n",
    "per_image[:, 4] /= per_image[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-image stats:\n",
    "# many prediction count (TP+FP)\n",
    "# many FP count\n",
    "# many FN count\n",
    "# TP + low conf + low IoU (avg on image)\n",
    "# FP + high conf (avg on image)\n",
    "# many rare classes (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_count = per_image[:, 0] + per_image[:, 1]\n",
    "# fp_count = per_image[:, 1]\n",
    "# fn_count = per_image[:, 2]\n",
    "\n",
    "\n",
    "# inds_sorted = np.argsort(prediction_count)[::-1]\n",
    "# prediction_count[inds_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP + high conf (avg on image)\n",
    "# Logarithmic Adjustment Score\n",
    "log_scores = np.log(1 + per_image[:, 1]) * per_image[:, 3]\n",
    "log_scores = np.nan_to_num(log_scores, nan=-1)\n",
    "inds_sorted = np.argsort(log_scores)[::-1]\n",
    "\n",
    "log_scores = log_scores[inds_sorted]\n",
    "img_ids = [idx2imgId[idx] for idx in inds_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many FN count\n",
    "fn_count = per_image[:, 2]\n",
    "inds_sorted = np.argsort(fn_count)[::-1]\n",
    "fn_count = fn_count[inds_sorted]\n",
    "img_ids = [idx2imgId[idx] for idx in inds_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low conf\n",
    "inds_sorted = np.argsort(per_image[:, 3])\n",
    "low_conf = per_image[:, 3][inds_sorted]\n",
    "img_ids = [idx2imgId[idx] for idx in inds_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low IoU\n",
    "inds_sorted = np.argsort(per_image[:, 4])\n",
    "low_iou = per_image[:, 4][inds_sorted]\n",
    "img_ids = [idx2imgId[idx] for idx in inds_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-instance stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increases score:\n",
    "# prediction with high confidence but incorrect (FP)\n",
    "# TP with low confidence, low IoU\n",
    "# edge cases: small objects, big objects, low IoU, low confidence\n",
    "# many predictions in one image\n",
    "# many FP predictions in one image\n",
    "# many FN in one image\n",
    "# miss-classified objects with high/low confidence\n",
    "# miss-classified prediciotn, and it is the most often miss-classified class\n",
    "\n",
    "# finding insightful FN:\n",
    "# many FN in one image\n",
    "# FN with small area\n",
    "# FN on the edge\n",
    "# FN with tall/len object\n",
    "\n",
    "# finding insightful TP:\n",
    "# TP with low confidence\n",
    "# TP with low IoU\n",
    "# TP with small area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-prediction stats:\n",
    "# TP + low conf\n",
    "# TP + low IoU\n",
    "# FP + high conf\n",
    "# rare class in GT\n",
    "# FN + small area (?)\n",
    "# miss-classified prediciotn + high/low conf\n",
    "# miss-classified prediciotn, and it is the most often miss-classified class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_matches = [match for match in matches if match[\"type\"] == \"TP\"]\n",
    "fp_matches = [match for match in matches if match[\"type\"] == \"FP\" and not match[\"miss_cls\"]]\n",
    "fn_matches = [match for match in matches if match[\"type\"] == \"FN\"]\n",
    "confused_matches = [match for match in matches if match[\"miss_cls\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP + low conf\n",
    "inds_sorted = np.argsort([match[\"score\"] for match in tp_matches])\n",
    "tp_low_conf_matches = [tp_matches[idx] for idx in inds_sorted]\n",
    "img_ids = [match[\"image_id\"] for match in tp_low_conf_matches]\n",
    "dt_ids = [match[\"dt_id\"] for match in tp_low_conf_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP + low IoU\n",
    "inds_sorted = np.argsort([match[\"iou\"] for match in tp_matches])\n",
    "tp_low_iou_matches = [tp_matches[idx] for idx in inds_sorted]\n",
    "img_ids = [match[\"image_id\"] for match in tp_low_iou_matches]\n",
    "dt_ids = [match[\"dt_id\"] for match in tp_low_iou_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP + high conf\n",
    "inds_sorted = np.argsort([match[\"score\"] for match in fp_matches])[::-1]\n",
    "fp_high_conf_matches = [fp_matches[idx] for idx in inds_sorted]\n",
    "img_ids = [match[\"image_id\"] for match in fp_high_conf_matches]\n",
    "dt_ids = [match[\"dt_id\"] for match in fp_high_conf_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confused + high/low conf\n",
    "inds_sorted = np.argsort([match[\"score\"] for match in confused_matches])[::1]\n",
    "confused_high_conf_matches = [confused_matches[idx] for idx in inds_sorted]\n",
    "img_ids = [match[\"image_id\"] for match in confused_high_conf_matches]\n",
    "dt_ids = [match[\"dt_id\"] for match in confused_high_conf_matches]\n",
    "gt_ids = [match[\"gt_id\"] for match in confused_high_conf_matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare classes\n",
    "\n",
    "Only for 3+ classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ids_rare, cat_names = utils.get_rare_classes(cocoGt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images with most rare classes\n",
    "\n",
    "counts = defaultdict(int)\n",
    "for match in matches:\n",
    "    if match[\"category_id\"] not in cat_ids_rare:\n",
    "        continue\n",
    "    counts[match[\"image_id\"]] += 1\n",
    "counts = dict(counts)\n",
    "\n",
    "# sort by count\n",
    "sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# img_id, n = sorted_counts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "utils.show_pred_image(sorted_counts[idx][0], cocoDt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_gt_image(img_ids[idx], cocoGt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/root/model-benchmark/utils.py'>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives, false_positives, false_negatives = utils.get_counts(cocoEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.587\n",
      "Recall: 0.538\n"
     ]
    }
   ],
   "source": [
    "tp = true_positives.sum(1)\n",
    "fp = false_positives.sum(1)\n",
    "fn = false_negatives.sum(1)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Precision: {precision.mean():.3f}\")\n",
    "print(f\"Recall: {recall.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46085.0"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([tp[0], fp[0], fn[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44425"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.619\n",
      "Recall: 0.546\n"
     ]
    }
   ],
   "source": [
    "tp = true_positives.sum((0,1))\n",
    "fp = false_positives.sum((0,1))\n",
    "fn = false_negatives.sum((0,1))\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Precision: {precision.mean():.3f}\")\n",
    "print(f\"Recall: {recall.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocoEval.params.iouThrs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
